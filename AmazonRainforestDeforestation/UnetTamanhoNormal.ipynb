{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarlosHenriqueMatos/neuralNetworksForCompetitions/blob/main/UnetTamanhoNormal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0YzZWGz-bpM",
        "outputId": "06fdba49-4a14-477c-e103-1f138e7eaffd"
      },
      "id": "l0YzZWGz-bpM",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "kWhb50dzHzX-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWhb50dzHzX-",
        "outputId": "2bce1d49-0b15-40a9-cb3c-6a84adac5009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "xbhxakPsHkQl",
      "metadata": {
        "id": "xbhxakPsHkQl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fa62163b",
      "metadata": {
        "id": "fa62163b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm  # Para uma barra de progresso\n",
        "\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "\n",
        "import keras.layers as layers\n",
        "from keras.models import Model\n",
        "#!pip install keras-adamw\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "# Don't Show Warning Messages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d5cd8454",
      "metadata": {
        "id": "d5cd8454"
      },
      "outputs": [],
      "source": [
        "IMG_HEIGHT = 512\n",
        "IMG_WIDTH = 512\n",
        "IMG_CHANNELS = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "B-t06Yu1zeDW",
      "metadata": {
        "id": "B-t06Yu1zeDW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a19d1af1",
      "metadata": {
        "id": "a19d1af1"
      },
      "outputs": [],
      "source": [
        "# get a list of files in each folder\n",
        "\n",
        "img_list = os.listdir('/content/drive/MyDrive/IFES_2023/AMAZON/Backup_Dataset/Training/image/')\n",
        "mask_list = os.listdir('/content/drive/MyDrive/IFES_2023/AMAZON/Backup_Dataset/Training/label/')\n",
        "\n",
        "# create a dataframe\n",
        "train_df_images = pd.DataFrame(img_list, columns=['image_id'])\n",
        "train_df_mask = pd.DataFrame(mask_list, columns=['image_id'])\n",
        "\n",
        "# get a list of files in each folder\n",
        "\n",
        "validation_img_list = os.listdir('/content/drive/MyDrive/IFES_2023/AMAZON/Backup_Dataset/Validation/images/')\n",
        "validation_mask_list = os.listdir('/content/drive/MyDrive/IFES_2023/AMAZON/Backup_Dataset/Validation/masks/')\n",
        "\n",
        "# create a dataframe\n",
        "test_df_images = pd.DataFrame(validation_img_list, columns=['image_id'])\n",
        "test_df_mask = pd.DataFrame(validation_mask_list, columns=['image_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f6cde629",
      "metadata": {
        "id": "f6cde629"
      },
      "outputs": [],
      "source": [
        "# Get lists of images and their masks.\n",
        "train_image_list = list(train_df_images['image_id'])\n",
        "train_mask_list = list(train_df_mask['image_id'])\n",
        "test_image_list = list(test_df_images['image_id'])\n",
        "test_mask_list = list(test_df_mask['image_id'])\n",
        "# Create empty arrays\n",
        "\n",
        "X_train = np.zeros((len(train_image_list), IMG_HEIGHT, IMG_WIDTH,3), dtype=np.float32)#You can use float16 or 32, enhance the precision\n",
        "Y_train = np.zeros((len(train_mask_list), IMG_HEIGHT, IMG_WIDTH), dtype=np.uint8)\n",
        "X_teste = np.zeros((len(test_image_list), IMG_HEIGHT, IMG_WIDTH,3), dtype=np.float32)\n",
        "Y_teste = np.zeros((len(test_mask_list), IMG_HEIGHT, IMG_WIDTH), dtype=np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = len(train_image_list)\n",
        "img_height, img_width, num_channels = 512, 512, 3\n",
        "\n",
        "# Pré-aloque o array\n",
        "X_train = np.zeros((num_images, img_height, img_width, num_channels), dtype=np.float32)\n",
        "\n",
        "for i, img_id in tqdm(enumerate(train_image_list)):\n",
        "    path_mask = '/content/drive/MyDrive/IFES_2023/AMAZON/Backup_Dataset/Training/image/' + img_id\n",
        "    image = cv2.imread(path_mask, 3)  # Usa a flag correta\n",
        "    X_train[i] = image  # Adiciona diretamente ao array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjTk_rOW_yvb",
        "outputId": "acac6e09-f00f-4792-f660-02feed436d38"
      },
      "id": "pjTk_rOW_yvb",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "499it [00:18, 27.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = len(train_image_list)\n",
        "img_height, img_width = 512, 512\n",
        "\n",
        "# Pré-aloque um array para as máscaras\n",
        "Y_train = np.zeros((num_images, img_height, img_width), dtype=np.uint8)\n",
        "\n",
        "for i, img_id in tqdm(enumerate(train_image_list)):\n",
        "    path_mask = '/content/drive/MyDrive/IFES_2023/AMAZON/Backup_Dataset/Training/label/' + img_id\n",
        "    image = cv2.imread(path_mask, 0)  # Ler como escala de cinza\n",
        "    Y_train[i] = image  # Adicionar diretamente ao array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oskt86FAdbq",
        "outputId": "69652a90-aee5-4ec7-c351-ec04f94396e9"
      },
      "id": "5oskt86FAdbq",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "499it [00:03, 138.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c73bf5b3",
      "metadata": {
        "id": "c73bf5b3"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "from tensorflow.keras.layers import Dropout, Lambda\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate, Lambda\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "x24USHOjBPUU",
      "metadata": {
        "id": "x24USHOjBPUU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plota_grafico(historico):\n",
        "    # figure(figsize = (8, 6))\n",
        "    ###################### losss\n",
        "    plt.plot(historico.history['loss'])\n",
        "    plt.plot(historico.history['val_loss'])\n",
        "    plt.title('LOSS')\n",
        "    plt.ylim(ymin = 0,ymax = 1)\n",
        "\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc ='upper left')\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    ###################### acuracia\n",
        "\n",
        "    plt.plot(historico.history['f1_score'])\n",
        "    plt.plot(historico.history['val_f1_score'])\n",
        "    plt.title('F1 Score')\n",
        "    plt.ylim(ymin = 0,ymax = 1)\n",
        "\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc ='upper left')\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFvOxA8hDqIC",
        "outputId": "6c026386-2c80-4dc6-b03c-8ec7dd1416db"
      },
      "id": "gFvOxA8hDqIC",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (3.5.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.5.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "VOaHajJvEKsT",
      "metadata": {
        "id": "VOaHajJvEKsT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy, Metric\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "XZ1fmI7bD22n",
      "metadata": {
        "id": "XZ1fmI7bD22n"
      },
      "outputs": [],
      "source": [
        "class F1Score(Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = Precision()\n",
        "        self.recall = Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        precision = self.precision.result()\n",
        "        recall = self.recall.result()\n",
        "        f1_score = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "        return f1_score\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_state()\n",
        "        self.recall.reset_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "qa5zc_oGFWN3",
      "metadata": {
        "id": "qa5zc_oGFWN3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_validate\n",
        "#from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# Importando os módulos de cálculo de métricas\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_train), type(Y_train))\n",
        "print(X_train.shape, Y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS8bMCMfHKe-",
        "outputId": "b6c8315b-8ac2-4fe8-ebc6-6c313e7bb406"
      },
      "id": "KS8bMCMfHKe-",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(499, 512, 512, 3) (499, 512, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N_9ahB8Ch3xE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_9ahB8Ch3xE",
        "outputId": "32a5bf22-bce8-488f-9c64-7299e5466d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        }
      ],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "import tensorflow as tf\n",
        "\n",
        "class KerasEstimator(BaseEstimator):\n",
        "    instance_counter = 0\n",
        "    def __init__(self, optimizer='adam', learning_rate=0.001, ema_momentum=0.9, kernel_initializer='he_normal'):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.ema_momentum = ema_momentum\n",
        "        self.optimizer = optimizer\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        KerasEstimator.instance_counter += 1  # Initialize the counter\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = self.create_model()\n",
        "\n",
        "        early_stopper = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='accuracy',  # Monitor the f1_score on the validation set\n",
        "            patience=10,\n",
        "            verbose=1,\n",
        "            restore_best_weights=False\n",
        "        )\n",
        "\n",
        "        '''checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=\"/content/drive/MyDrive/Mestrado/2024_2/Qualification/amazonModels/carlos.h5\",\n",
        "            monitor='accuracy',  # Monitor the f1_score on the validation set\n",
        "            verbose=1,\n",
        "            save_best_only=True,\n",
        "            mode='max'\n",
        "        )'''\n",
        "\n",
        "        history = self.model.fit(X, y, batch_size=16, epochs=3, verbose=1,callbacks=[early_stopper])\n",
        "\n",
        "        print(history.history['accuracy'])\n",
        "        with open(\"/content/drive/MyDrive/Mestrado/2024_2/Qualification/amazonModels/treinos1e2.txt\", \"a\") as checkpoint_file:\n",
        "          checkpoint_file.write(\"Accuracy History: \" + str(history.history['accuracy']) + \"\\n\")\n",
        "\n",
        "        print(\"Dados Gravados\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def create_model(self):\n",
        "        #print(self.ema_momentum,self.learning_rate,self.kernel_initializer,self.optimizer)\n",
        "        inputs = Input((IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "\n",
        "        s = tf.keras.layers.Lambda(lambda x: x / 65.536) (inputs)\n",
        "\n",
        "        c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same') (s)\n",
        "        c1 = tf.keras.layers.Dropout(0.1) (c1)\n",
        "        c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same') (c1)\n",
        "        p1 = tf.keras.layers.MaxPooling2D((2, 2)) (c1)\n",
        "\n",
        "        c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same') (p1)\n",
        "        c2 = tf.keras.layers.Dropout(0.1) (c2)\n",
        "        c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same') (c2)\n",
        "        p2 = tf.keras.layers.MaxPooling2D((2, 2)) (c2)\n",
        "\n",
        "        c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same') (p2)\n",
        "        c3 = tf.keras.layers.Dropout(0.2) (c3)\n",
        "        c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same') (c3)\n",
        "        p3 = tf.keras.layers.MaxPooling2D((2, 2)) (c3)\n",
        "\n",
        "        c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same') (p3)\n",
        "        c4 = tf.keras.layers.Dropout(0.2) (c4)\n",
        "        c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same') (c4)\n",
        "        p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "\n",
        "        c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same') (p4)\n",
        "        c5 = tf.keras.layers.Dropout(0.3) (c5)\n",
        "        c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same') (c5)\n",
        "\n",
        "        u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "        u6 = tf.keras.layers.Concatenate()([u6, c4])  # Fix the Concatenate usage\n",
        "        c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same')(u6)\n",
        "        c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
        "        c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same')(c6)\n",
        "\n",
        "        u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "        u7 = tf.keras.layers.Concatenate()([u7, c3])  # Fix the Concatenate usage\n",
        "        c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same')(u7)\n",
        "        c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
        "        c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same')(c7)\n",
        "\n",
        "        u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "        u8 = tf.keras.layers.Concatenate()([u8, c2])  # Fix the Concatenate usage\n",
        "        c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same')(u8)\n",
        "        c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
        "        c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same')(c8)\n",
        "\n",
        "\n",
        "        u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "        u9 = tf.keras.layers.Concatenate(axis=3)([u9, c1])  # Fix the Concatenate usage\n",
        "        c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same')(u9)\n",
        "        c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
        "        c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer=self.kernel_initializer, padding='same')(c9)\n",
        "\n",
        "        outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
        "\n",
        "        model = Model(inputs=[inputs], outputs=[outputs])  # Your model architecture here\n",
        "        model.compile(optimizer=self.get_optimizer(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        if self.optimizer == 'adam':\n",
        "            return tf.keras.optimizers.Adam(learning_rate=self.learning_rate, ema_momentum=self.ema_momentum)\n",
        "        elif self.optimizer == 'nadam':\n",
        "            return tf.keras.optimizers.Nadam(learning_rate=self.learning_rate, ema_momentum=self.ema_momentum)\n",
        "        elif self.optimizer == 'adamw':\n",
        "            # Implement AdamW optimizer (adjust parameters as needed)\n",
        "            return tf.keras.optimizers.AdamW(learning_rate=self.learning_rate, ema_momentum=self.ema_momentum)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown optimizer: {}\".format(self.optimizer))\n",
        "\n",
        "# Define hyperparameters for grid search\n",
        "parameters = {\n",
        "    'optimizer':['adam'], #['adam','nadam','adamw'],\n",
        "    'learning_rate': [0.002,0.003,0.004],\n",
        "    'ema_momentum': [0.975],\n",
        "    'kernel_initializer': ['he_normal']\n",
        "}\n",
        "\n",
        "# Create the wrapped KerasEstimator\n",
        "#cv = KFold(n_splits=5, shuffle=True, random_state=42)  # Use 5-fold cross-validation\n",
        "clf = GridSearchCV(estimator = KerasEstimator(), param_grid=parameters,scoring = 'f1',cv=5)\n",
        "history = clf.fit(X_train, Y_train)\n",
        "# Fit the grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yYQJYpJ_JEe7",
      "metadata": {
        "id": "yYQJYpJ_JEe7"
      },
      "outputs": [],
      "source": [
        "# Acesse os melhores parâmetros encontrados no grid_result\n",
        "best_params = grid_result.best_params_\n",
        "best_score = grid_result.best_score_\n",
        "\n",
        "print(\"Melhores parâmetros encontrados: \", best_params)\n",
        "print(\"Melhor precisão encontrada: \", best_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B9n3Z2DqBRyv",
      "metadata": {
        "id": "B9n3Z2DqBRyv"
      },
      "outputs": [],
      "source": [
        "modelo = 0\n",
        "model = unet()\n",
        "for train, test in cv.split(X_train,Y_train):\n",
        "    modelo=modelo+1\n",
        "    print(modelo)\n",
        "    fold_no = 1\n",
        "    acc_per_fold = []\n",
        "    filepath = \"/content/drive/MyDrive/AMAZON/TesteUnet/Unet_Adam_1e-3_10P_he_0.90_/Unet_Adam_1e-3_10P_he_0.90_\"+str(modelo)+\".h5\"\n",
        "\n",
        "    earlystopper = EarlyStopping(patience=10, verbose=1)\n",
        "\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='val_f1_score', verbose=1,\n",
        "                                  save_best_only=True, mode='max')\n",
        "\n",
        "    callbacks_list = [earlystopper,checkpoint]\n",
        "\n",
        "    X = X_train[train]\n",
        "    Y = Y_train[train]\n",
        "\n",
        "    Xt = X_train[test]\n",
        "    Yt = Y_train[test]\n",
        "\n",
        "    model = unet()\n",
        "    print(\"Modelo\")\n",
        "    history=0\n",
        "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3,ema_momentum=0.9),\n",
        "                  metrics=[BinaryAccuracy(), F1Score()])\n",
        "\n",
        "    history = model.fit(X, Y, batch_size=8,validation_data=(Xt, Yt), epochs = 1000,\n",
        "                    callbacks=callbacks_list,shuffle=False)\n",
        "\n",
        "    plota_grafico(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nP9daxUOlYOX",
      "metadata": {
        "id": "nP9daxUOlYOX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3d58afe",
      "metadata": {
        "id": "b3d58afe",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "dir = \"/content/drive/MyDrive/AMAZON/TestesIsolados/\"\n",
        "diretorio = os.listdir(dir)\n",
        "model = unet()\n",
        "todasMatrizes=[]\n",
        "print(diretorio)\n",
        "for i in range(len(diretorio)):\n",
        "  model.load_weights(dir+diretorio[i])\n",
        "  matrizes=[]\n",
        "  matrizes2=[[0,0],[0,0]]\n",
        "  amostras = 45\n",
        "  index=0\n",
        "  for _ in range(amostras):\n",
        "\n",
        "      img = cv2.imread('/content/drive/MyDrive/AMAZON/Dataset/Test/image/'+test_image_list[index],3)\n",
        "      img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
        "      img = img[np.newaxis, :, :, : ]\n",
        "\n",
        "      predicted_img = model.predict(img)\n",
        "      cv2.imwrite('/content/drive/MyDrive/AMAZON/Dataset/Test/'+test_image_list[index], predicted_img)\n",
        "\n",
        "      #plt.figure(figsize=(12, 12))\n",
        "      #image = cv2.imread('/content/drive/MyDrive/AMAZON/Dataset/Test/image/'+test_image_list[index], 2)\n",
        "      #image=image*255\n",
        "      #plt.subplot(1, 3, 1)\n",
        "      #plt.imshow(np.squeeze(image))\n",
        "      #plt.axis('off')\n",
        "      #plt.title('Original Image')\n",
        "\n",
        "      #plt.subplot(1, 3, 2)\n",
        "      #plt.imshow(np.squeeze(cv2.imread('/content/drive/MyDrive/AMAZON/Dataset/Test/mask/'+test_image_list[index],0)))\n",
        "      #plt.axis('off')\n",
        "      #plt.title('Original Mask')\n",
        "      list_arr = cv2.imread('/content/drive/MyDrive/AMAZON/Dataset/Test/mask/'+test_image_list[index],0).tolist()\n",
        "\n",
        "      #print(list_arr[0])\n",
        "\n",
        "\n",
        "      #plt.subplot(1, 3, 3)\n",
        "      #plt.imshow(np.squeeze(predicted_img) >= 0.5 )\n",
        "      #plt.title('Prediction')\n",
        "      #plt.axis('off')\n",
        "      predicted_img=(np.squeeze(predicted_img)>=0.5)\n",
        "\n",
        "\n",
        "      # assuming the array is named 'arr'\n",
        "      predicted_img = predicted_img.astype(np.int64)\n",
        "      list_arr2 = predicted_img.tolist()\n",
        "      #print(list_arr2[0])\n",
        "      for i in range(len(list_arr)):\n",
        "        cm = confusion_matrix(list_arr[i],list_arr2[i])\n",
        "        if cm.shape == (2, 2):\n",
        "          matrizes2[0][0]+=cm[0][0]\n",
        "          matrizes2[0][1]+=cm[0][1]\n",
        "          matrizes2[1][0]+=cm[1][0]\n",
        "          matrizes2[1][1]+=cm[1][1]\n",
        "          teste=np.cumsum(matrizes2)\n",
        "      #print(teste[3])\n",
        "      print(matrizes2[0]/teste[3],'\\n',matrizes2[1]/teste[3])\n",
        "      matrizes2=[[0,0],[0,0]]\n",
        "      matrizes.append(matrizes2)\n",
        "      plt.show()\n",
        "      index += 1\n",
        "      #print(index)\n",
        "      print(index)\n",
        "  todasMatrizes.append(matrizes)\n",
        "  print(todasMatrizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "877e3c69",
      "metadata": {
        "id": "877e3c69"
      },
      "outputs": [],
      "source": [
        "sum_confusion_matrix = np.zeros_like(matrizes[0])\n",
        "\n",
        "# iterate over all matrices and add them to the sum matrix\n",
        "for matrizes in todasMatrizes:\n",
        "  for matrix in matrizes:\n",
        "      sum_confusion_matrix += matrix\n",
        "val = sum_confusion_matrix[0][0]+sum_confusion_matrix[0][1]\n",
        "val1 = sum_confusion_matrix[1][0]+sum_confusion_matrix[1][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qqVVXGzpg_T-",
      "metadata": {
        "id": "qqVVXGzpg_T-"
      },
      "outputs": [],
      "source": [
        "TP=sum_confusion_matrix[0][0]\n",
        "FP=sum_confusion_matrix[0][1]\n",
        "FN=sum_confusion_matrix[1][0]\n",
        "TN=sum_confusion_matrix[1][1]\n",
        "print(TP,FP,FN,TN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fV5xzItphC2l",
      "metadata": {
        "id": "fV5xzItphC2l"
      },
      "outputs": [],
      "source": [
        "precisao = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "F1_Score = 2 * (precisao * recall) / (precisao + recall)\n",
        "acuracia = (TP+TN) / (TN+TP+FP+FN)\n",
        "iou = TP / (TP + FP + FN)\n",
        "print(\"Precisão: \",precisao,\"\\nRecall: \",recall,\"\\nF1_Score: \",F1_Score,\"\\nAcuracia: \",acuracia,\"\\nIoU: \",iou)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SpLjH0INhH2E",
      "metadata": {
        "id": "SpLjH0INhH2E"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "matriz213 =[[TP/(TP + FP),FP/(TP + FP)],[FN/(FN+TN),TN/(FN+TN)]]\n",
        "sns.heatmap(matriz213, annot=True,fmt=\".3f\", annot_kws={\"size\":12}, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wZ1mjyqd62CT",
      "metadata": {
        "id": "wZ1mjyqd62CT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}