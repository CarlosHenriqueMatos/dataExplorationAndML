{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26uIl2XGkgcN"
      },
      "source": [
        "\n",
        "# **Image Colorization With GANs**\n",
        "\n",
        "GANs are the state-of-the-art machine learning models which can generate new data instances from existing ones. They use a very interesting technique, inspired from the Game Theory, to generate realistic samples.\n",
        "\n",
        "In this notebook, we'll use GANs to colorize a grayscale ( B/W ) image. In addition to that, our generator model will have a structure similar to that of a UNet i.e the one with skip connections.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4-cYBxxmjK4"
      },
      "source": [
        "\n",
        "## **1. Downloading and Processing the data**\n",
        "\n",
        "A dataset of RGB images to train the GAN model whose images consists of various scenes/places.\n",
        "\n",
        "* Download the dataset on your machine from [here](https://drive.google.com/file/d/1sQ5C8HiKVr2Edp3ojLLNauwRbOLfVn2q/view?usp=sharing).\n",
        "* Upload the downloaded `.zip` file here on Colab.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfLiBjvlJeUl",
        "outputId": "f9745caa-1d07-478f-cfdc-27b986e78996"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLbLh8Y2od4P"
      },
      "source": [
        "\n",
        "We'll now parse the images ( RGB images to be precise ) one by one, and transform each one to a grayscale image using PIL's `.convert( 'L' )` method. So our dataset will have samples of $( \\ grayscale \\ image \\ , \\ RGB \\ image \\ )$\n",
        "\n",
        "We used only a part of our dataset, determined by `dataset_split` , as Colab's computational power would cease on providing a large number of images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4Y9h0CzfL91"
      },
      "source": [
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import image\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "from tensorflow import keras\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "img_size = 128\n",
        "\n",
        "dataset_split = 600\n",
        "\n",
        "master_dir = '/content/drive/MyDrive/Mestrado/2024_2/Topicos_especiais_IA_ThiagoPX/NormalAndMonet'\n",
        "x = []\n",
        "y = []\n",
        "for image_file in os.listdir( master_dir )[ 0 : dataset_split ]:\n",
        "    rgb_image = Image.open( os.path.join( master_dir , image_file ) ).resize( ( img_size , img_size ) )\n",
        "\n",
        "    rgb_img_array = (np.asarray( rgb_image ) ) / 255\n",
        "    gray_image = rgb_image.convert( 'L' )\n",
        "\n",
        "    gray_img_array = ( np.asarray( gray_image ).reshape( ( img_size , img_size , 1 ) ) ) / 255\n",
        "\n",
        "    x.append( gray_img_array )\n",
        "    y.append( rgb_img_array )\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split( np.array(x) , np.array(y) , test_size=0.1 )\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices( ( train_x , train_y ) )\n",
        "dataset = dataset.batch( batch_size )\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Parameters\n",
        "batch_size = 64\n",
        "img_size = 128\n",
        "dataset_split = 600\n",
        "master_dir = '/content/drive/MyDrive/Mestrado/2024_2/Topicos_especiais_IA_ThiagoPX/NormalAndMonet'\n",
        "\n",
        "# Image loading and preprocessing\n",
        "def load_and_preprocess_image(image_path):\n",
        "    # Load RGB image\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [img_size, img_size])\n",
        "    img = img / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray_img = tf.image.rgb_to_grayscale(img)\n",
        "    return gray_img, img\n",
        "\n",
        "# Create dataset\n",
        "image_files = [os.path.join(master_dir, fname) for fname in os.listdir(master_dir)[:dataset_split]]\n",
        "dataset = tf.data.Dataset.from_tensor_slices(image_files)\n",
        "\n",
        "# Map the preprocessing function\n",
        "dataset = dataset.map(lambda x: load_and_preprocess_image(x))\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "train_size = int(0.9 * len(image_files))  # 90% for training\n",
        "train_dataset = dataset.take(train_size)\n",
        "test_dataset = dataset.skip(train_size)\n",
        "\n",
        "# Batch the datasets\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "\n",
        "# Prefetch for performance optimization\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Print to check the datasets\n",
        "print(f\"Train Dataset: {len(list(train_dataset))} batches\")\n",
        "print(f\"Test Dataset: {len(list(test_dataset))} batches\")\n"
      ],
      "metadata": {
        "id": "3juHPenx8KxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euCdKKt_r4wg"
      },
      "source": [
        "\n",
        "## **2. The GAN**\n",
        "\n",
        "In this section, we'll create our GAN model step-by-step with Keras. First, we'll implement the generator then the discriminator and finally the loss functions required by both of them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQUjuTsJsbwK"
      },
      "source": [
        "\n",
        "### **A. Generator**\n",
        "\n",
        "Our generator ( represented as $G$ ) will take in grayscale image $x$ and produce a RGB image $G( x )$. Note, $x$ will be a tensor of shape $( \\ batch \\ size \\ , \\ 120 \\ , \\ 120 \\ , \\ 1 \\ )$ and the output $G(x)$ will have a shape $( \\ batch \\ size \\ , \\ 120 \\ , \\ 120 \\ , \\ 3 \\ )$\n",
        "\n",
        "* Our generator will have a encoder-decoder structure, similar to the UNet architecture. Additionally, we use Dilated convolutions to have a larger receptive field.\n",
        "\n",
        "* We introduce skip connections in our model so as to have better flow of information from the encoder to the decoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIZEoRpB03YS"
      },
      "source": [
        "def get_generator_model():\n",
        "    inputs = tf.keras.layers.Input(shape=(img_size, img_size, 1))\n",
        "\n",
        "    # Encoder com Dropout\n",
        "    conv1 = tf.keras.layers.Conv2D(16, kernel_size=(5, 5), strides=1)(inputs)\n",
        "    conv1 = tf.keras.layers.LeakyReLU()(conv1)\n",
        "    conv1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=1)(conv1)\n",
        "    conv1 = tf.keras.layers.LeakyReLU()(conv1)\n",
        "    conv1 = tf.keras.layers.Dropout(0.3)(conv1)\n",
        "    conv1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=1)(conv1)\n",
        "    conv1 = tf.keras.layers.LeakyReLU()(conv1)\n",
        "\n",
        "    conv2 = tf.keras.layers.Conv2D(32, kernel_size=(5, 5), strides=1)(conv1)\n",
        "    conv2 = tf.keras.layers.LeakyReLU()(conv2)\n",
        "    conv2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=1)(conv2)\n",
        "    conv2 = tf.keras.layers.LeakyReLU()(conv2)\n",
        "    conv2 = tf.keras.layers.Dropout(0.3)(conv2)\n",
        "    conv2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=1)(conv2)\n",
        "    conv2 = tf.keras.layers.LeakyReLU()(conv2)\n",
        "\n",
        "    conv3 = tf.keras.layers.Conv2D(64, kernel_size=(5, 5), strides=1)(conv2)\n",
        "    conv3 = tf.keras.layers.LeakyReLU()(conv3)\n",
        "    conv3 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=1)(conv3)\n",
        "    conv3 = tf.keras.layers.LeakyReLU()(conv3)\n",
        "    conv3 = tf.keras.layers.Dropout(0.3)(conv3)\n",
        "    conv3 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=1)(conv3)\n",
        "    conv3 = tf.keras.layers.LeakyReLU()(conv3)\n",
        "\n",
        "    # Bottleneck\n",
        "    bottleneck = tf.keras.layers.Conv2D(256, kernel_size=(3, 3), strides=1, activation='tanh', padding='same')(conv3)\n",
        "\n",
        "    # Decoder com Dropout\n",
        "    concat_1 = tf.keras.layers.Concatenate()([bottleneck, conv3])\n",
        "    conv_up_3 = tf.keras.layers.Conv2DTranspose(128, kernel_size=(3, 3), strides=1, activation='relu')(concat_1)\n",
        "    conv_up_3 = tf.keras.layers.Conv2DTranspose(128, kernel_size=(3, 3), strides=1, activation='relu')(conv_up_3)\n",
        "    conv_up_3 = tf.keras.layers.Dropout(0.3)(conv_up_3)\n",
        "    conv_up_3 = tf.keras.layers.Conv2DTranspose(64, kernel_size=(5, 5), strides=1, activation='relu')(conv_up_3)\n",
        "\n",
        "    concat_2 = tf.keras.layers.Concatenate()([conv_up_3, conv2])\n",
        "    conv_up_2 = tf.keras.layers.Conv2DTranspose(64, kernel_size=(3, 3), strides=1, activation='relu')(concat_2)\n",
        "    conv_up_2 = tf.keras.layers.Conv2DTranspose(64, kernel_size=(3, 3), strides=1, activation='relu')(conv_up_2)\n",
        "    conv_up_2 = tf.keras.layers.Dropout(0.3)(conv_up_2)\n",
        "    conv_up_2 = tf.keras.layers.Conv2DTranspose(32, kernel_size=(5, 5), strides=1, activation='relu')(conv_up_2)\n",
        "\n",
        "    concat_3 = tf.keras.layers.Concatenate()([conv_up_2, conv1])\n",
        "    conv_up_1 = tf.keras.layers.Conv2DTranspose(32, kernel_size=(3, 3), strides=1, activation='relu')(concat_3)\n",
        "    conv_up_1 = tf.keras.layers.Conv2DTranspose(32, kernel_size=(3, 3), strides=1, activation='relu')(conv_up_1)\n",
        "    conv_up_1 = tf.keras.layers.Conv2DTranspose(3, kernel_size=(5, 5), strides=1, activation='relu')(conv_up_1)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs, conv_up_1)\n",
        "    return model\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHPoEyuiujv1"
      },
      "source": [
        "\n",
        "### **B. Discriminator**\n",
        "\n",
        "The discriminator model, represented as $D$, will take in the *real image* $y$ ( from the training data ) and the *generated image* $G(x)$ ( from the generator ) to output two probabilities.\n",
        "\n",
        "* We train the discriminator in such a manner that is able to differentiate the *real images* and the generated *images*. So, we train the model such that $y$ produces a output of $1.0$ and $G(x)$ produces an output of $0.0$.\n",
        "* Note that instead of using hard labels like $1.0$ and $0.0$, we use soft labels which are close to 1 and 0. So for a hard label of $1.0$, the soft label will be $(1 - \\epsilon)$ where $\\epsilon$ is picked uniformly from $( 0 , 0.1 ]$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MN4QA16xSbi"
      },
      "source": [
        "def get_discriminator_model():\n",
        "    layers = [\n",
        "        tf.keras.layers.Conv2D(32, kernel_size=(7, 7), strides=1, activation='relu', input_shape=(img_size, img_size, 3)),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Conv2D(32, kernel_size=(7, 7), strides=1, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Conv2D(64, kernel_size=(5, 5), strides=1, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Conv2D(64, kernel_size=(5, 5), strides=1, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=1, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=1, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Conv2D(256, kernel_size=(3, 3), strides=1, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Conv2D(256, kernel_size=(3, 3), strides=1, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ]\n",
        "    model = tf.keras.models.Sequential(layers)\n",
        "    return model\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CER2JcDt08G8"
      },
      "source": [
        "\n",
        "### **C. Loss Functions**\n",
        "\n",
        "We'll now implement the loss functions for our GAN model. As you might know that we have two loss functions, one for the generator and another for the discriminator.\n",
        "\n",
        "* For our generator, we'll use the L2/MSE loss function.\n",
        "* For optimization, we use the Adam optimizer with a learning rate of 0.0005\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXLB6fR4fFQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4525058-e5b8-46e0-add4-a42a19f6ef3c"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "\n",
        "    real_labels = tf.ones_like(real_output) - tf.random.uniform(shape=real_output.shape, maxval=0.1)\n",
        "    fake_labels = tf.zeros_like(fake_output) + tf.random.uniform(shape=fake_output.shape, maxval=0.1)\n",
        "\n",
        "    real_loss = cross_entropy(real_labels, real_output)\n",
        "    fake_loss = cross_entropy(fake_labels, fake_output)\n",
        "\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output, real_y):\n",
        "    \"\"\"\n",
        "    Calcula a perda do gerador usando o erro médio quadrático (MSE).\n",
        "    \"\"\"\n",
        "    real_y = tf.cast(real_y, tf.float32)\n",
        "    return mse(fake_output, real_y)\n",
        "\n",
        "learning_rate = 0.001\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5, beta_2=0.999)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5, beta_2=0.999)\n",
        "\n",
        "generator = get_generator_model()\n",
        "discriminator = get_discriminator_model()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXgwdqAHd_ar"
      },
      "source": [
        "\n",
        "## **3. Training The GAN**\n",
        "\n",
        "So finally, we'll train our GAN on the dataset, we prepared earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxUEQpDmgoLa"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "gen_loss_metric = tf.keras.metrics.Mean(name=\"gen_loss\")\n",
        "disc_loss_metric = tf.keras.metrics.Mean(name=\"disc_loss\")\n",
        "\n",
        "early_stopping_patience = 10\n",
        "best_gen_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "@tf.function\n",
        "def train_step(input_x, real_y):\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(input_x, training=True)\n",
        "\n",
        "        real_output = discriminator(real_y, training=True)\n",
        "        generated_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(generated_images, real_y)\n",
        "        disc_loss = discriminator_loss(real_output, generated_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    gen_loss_metric.update_state(gen_loss)\n",
        "    disc_loss_metric.update_state(disc_loss)\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    \"\"\"\n",
        "    Loop principal de treinamento.\n",
        "    \"\"\"\n",
        "    global best_gen_loss, patience_counter\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        gen_loss_metric.reset_state()\n",
        "        disc_loss_metric.reset_state()\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        progress_bar = tqdm(dataset, desc=\"Training\", leave=True)\n",
        "\n",
        "        for input_x, real_y in progress_bar:\n",
        "            train_step(input_x, real_y)\n",
        "            progress_bar.set_postfix({\n",
        "                \"Generator Loss\": f\"{gen_loss_metric.result():.4f}\",\n",
        "                \"Discriminator Loss\": f\"{disc_loss_metric.result():.4f}\"\n",
        "            })\n",
        "\n",
        "        print(f\"Generator Loss: {gen_loss_metric.result():.4f}, Discriminator Loss: {disc_loss_metric.result():.4f}\")\n",
        "\n",
        "        current_gen_loss = gen_loss_metric.result()\n",
        "        if current_gen_loss < best_gen_loss:\n",
        "            best_gen_loss = current_gen_loss\n",
        "            patience_counter = 0\n",
        "            print(\"Validation improvement. Saving best model...\\n\")\n",
        "            generator.save_weights(\"/content/drive/MyDrive/Mestrado/2024_2/Topicos_especiais_IA_ThiagoPX/best_generator.weights.h5\")\n",
        "            discriminator.save_weights(\"/content/drive/MyDrive/Mestrado/2024_2/Topicos_especiais_IA_ThiagoPX/best_discriminator.weights.h5\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"No improvement. Early stopping patience: {patience_counter}/{early_stopping_patience}\")\n",
        "\n",
        "\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered. Stopping training.\")\n",
        "            break\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoQ9H3Iwfa_x"
      },
      "source": [
        "\n",
        "\n",
        "Run the cell below, to start the training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(dataset=dataset, epochs=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eftrBW0tyjyt",
        "outputId": "556cc947-a691-4982-f4e0-25f93fb62d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [01:35<00:00, 10.60s/it, Generator Loss=0.2754, Discriminator Loss=1.8354]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.2754, Discriminator Loss: 1.8354\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 2/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.85s/it, Generator Loss=0.0367, Discriminator Loss=1.3886]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0367, Discriminator Loss: 1.3886\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 3/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.84s/it, Generator Loss=0.0335, Discriminator Loss=1.3851]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0335, Discriminator Loss: 1.3851\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 4/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0334, Discriminator Loss=1.3921]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0334, Discriminator Loss: 1.3921\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 5/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.87s/it, Generator Loss=0.0264, Discriminator Loss=1.3892]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0264, Discriminator Loss: 1.3892\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 6/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0228, Discriminator Loss=1.3865]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0228, Discriminator Loss: 1.3865\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 7/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.85s/it, Generator Loss=0.0212, Discriminator Loss=1.3874]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0212, Discriminator Loss: 1.3874\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 8/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0172, Discriminator Loss=1.3866]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0172, Discriminator Loss: 1.3866\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 9/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.85s/it, Generator Loss=0.0170, Discriminator Loss=1.3868]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0170, Discriminator Loss: 1.3868\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 10/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.89s/it, Generator Loss=0.0148, Discriminator Loss=1.3882]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0148, Discriminator Loss: 1.3882\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 11/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0156, Discriminator Loss=1.3868]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0156, Discriminator Loss: 1.3868\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 12/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0137, Discriminator Loss=1.4395]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0137, Discriminator Loss: 1.4395\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 13/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0141, Discriminator Loss=1.3857]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0141, Discriminator Loss: 1.3857\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 14/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0137, Discriminator Loss=1.3934]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0137, Discriminator Loss: 1.3934\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 15/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0135, Discriminator Loss=1.3867]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0135, Discriminator Loss: 1.3867\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 16/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0133, Discriminator Loss=1.3861]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0133, Discriminator Loss: 1.3861\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 17/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.87s/it, Generator Loss=0.0132, Discriminator Loss=1.3865]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0132, Discriminator Loss: 1.3865\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 18/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0131, Discriminator Loss=1.3862]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0131, Discriminator Loss: 1.3862\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 19/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0130, Discriminator Loss=1.3865]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0130, Discriminator Loss: 1.3865\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 20/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0128, Discriminator Loss=1.3863]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0128, Discriminator Loss: 1.3863\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 21/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:17<00:00,  1.89s/it, Generator Loss=0.0126, Discriminator Loss=1.3861]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0126, Discriminator Loss: 1.3861\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 22/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0124, Discriminator Loss=1.3861]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0124, Discriminator Loss: 1.3861\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 23/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.84s/it, Generator Loss=0.0130, Discriminator Loss=1.3867]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0130, Discriminator Loss: 1.3867\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 24/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0138, Discriminator Loss=1.3872]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0138, Discriminator Loss: 1.3872\n",
            "No improvement. Early stopping patience: 2/10\n",
            "\n",
            "Epoch 25/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:17<00:00,  1.89s/it, Generator Loss=0.0131, Discriminator Loss=1.3862]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0131, Discriminator Loss: 1.3862\n",
            "No improvement. Early stopping patience: 3/10\n",
            "\n",
            "Epoch 26/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0124, Discriminator Loss=1.3860]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0124, Discriminator Loss: 1.3860\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 27/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.84s/it, Generator Loss=0.0121, Discriminator Loss=1.3862]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0121, Discriminator Loss: 1.3862\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 28/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.85s/it, Generator Loss=0.0120, Discriminator Loss=1.3859]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0120, Discriminator Loss: 1.3859\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 29/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0120, Discriminator Loss=1.3866]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0120, Discriminator Loss: 1.3866\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 30/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0117, Discriminator Loss=1.3859]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0117, Discriminator Loss: 1.3859\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 31/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0118, Discriminator Loss=1.3860]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0118, Discriminator Loss: 1.3860\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 32/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.85s/it, Generator Loss=0.0114, Discriminator Loss=1.3865]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0114, Discriminator Loss: 1.3865\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 33/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0119, Discriminator Loss=1.3874]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0119, Discriminator Loss: 1.3874\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 34/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0113, Discriminator Loss=1.3858]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0113, Discriminator Loss: 1.3858\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 35/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.84s/it, Generator Loss=0.0117, Discriminator Loss=1.3868]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0117, Discriminator Loss: 1.3868\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 36/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0112, Discriminator Loss=1.3874]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0112, Discriminator Loss: 1.3874\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 37/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0115, Discriminator Loss=1.3870]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0115, Discriminator Loss: 1.3870\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 38/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0114, Discriminator Loss=1.3856]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0114, Discriminator Loss: 1.3856\n",
            "No improvement. Early stopping patience: 2/10\n",
            "\n",
            "Epoch 39/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0110, Discriminator Loss=1.3872]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0110, Discriminator Loss: 1.3872\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 40/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.85s/it, Generator Loss=0.0113, Discriminator Loss=1.3857]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0113, Discriminator Loss: 1.3857\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 41/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0111, Discriminator Loss=1.3857]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0111, Discriminator Loss: 1.3857\n",
            "No improvement. Early stopping patience: 2/10\n",
            "\n",
            "Epoch 42/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0111, Discriminator Loss=1.3871]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0111, Discriminator Loss: 1.3871\n",
            "No improvement. Early stopping patience: 3/10\n",
            "\n",
            "Epoch 43/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0109, Discriminator Loss=1.3870]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0109, Discriminator Loss: 1.3870\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 44/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.87s/it, Generator Loss=0.0111, Discriminator Loss=1.3863]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0111, Discriminator Loss: 1.3863\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 45/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0109, Discriminator Loss=1.3861]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0109, Discriminator Loss: 1.3861\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 46/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.85s/it, Generator Loss=0.0109, Discriminator Loss=1.3862]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0109, Discriminator Loss: 1.3862\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 47/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0108, Discriminator Loss=1.3865]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0108, Discriminator Loss: 1.3865\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 48/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0108, Discriminator Loss=1.3861]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0108, Discriminator Loss: 1.3861\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 49/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.85s/it, Generator Loss=0.0110, Discriminator Loss=1.3860]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0110, Discriminator Loss: 1.3860\n",
            "No improvement. Early stopping patience: 2/10\n",
            "\n",
            "Epoch 50/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0106, Discriminator Loss=1.3867]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0106, Discriminator Loss: 1.3867\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 51/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0108, Discriminator Loss=1.3860]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0108, Discriminator Loss: 1.3860\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 52/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0106, Discriminator Loss=1.3856]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0106, Discriminator Loss: 1.3856\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 53/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0107, Discriminator Loss=1.3862]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0107, Discriminator Loss: 1.3862\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 54/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0105, Discriminator Loss=1.3857]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0105, Discriminator Loss: 1.3857\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 55/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.85s/it, Generator Loss=0.0105, Discriminator Loss=1.3874]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0105, Discriminator Loss: 1.3874\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 56/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0105, Discriminator Loss=1.3862]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0105, Discriminator Loss: 1.3862\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 57/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0105, Discriminator Loss=1.3854]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0105, Discriminator Loss: 1.3854\n",
            "No improvement. Early stopping patience: 2/10\n",
            "\n",
            "Epoch 58/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0103, Discriminator Loss=1.3862]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0103, Discriminator Loss: 1.3862\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 59/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.87s/it, Generator Loss=0.0103, Discriminator Loss=1.3859]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0103, Discriminator Loss: 1.3859\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 60/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0104, Discriminator Loss=1.3876]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0104, Discriminator Loss: 1.3876\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 61/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0102, Discriminator Loss=1.3860]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0102, Discriminator Loss: 1.3860\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 62/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0102, Discriminator Loss=1.3871]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0102, Discriminator Loss: 1.3871\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 63/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0103, Discriminator Loss=1.3874]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0103, Discriminator Loss: 1.3874\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 64/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0103, Discriminator Loss=1.3871]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0103, Discriminator Loss: 1.3871\n",
            "No improvement. Early stopping patience: 2/10\n",
            "\n",
            "Epoch 65/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:20<00:00,  2.27s/it, Generator Loss=0.0101, Discriminator Loss=1.3869]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0101, Discriminator Loss: 1.3869\n",
            "Validation improvement. Saving best model...\n",
            "\n",
            "\n",
            "Epoch 66/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 9/9 [00:16<00:00,  1.86s/it, Generator Loss=0.0102, Discriminator Loss=1.3863]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.0102, Discriminator Loss: 1.3863\n",
            "No improvement. Early stopping patience: 1/10\n",
            "\n",
            "Epoch 67/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  44%|████▍     | 4/9 [00:08<00:10,  2.02s/it, Generator Loss=0.0101, Discriminator Loss=1.3871]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynE-Hp6lWeZT"
      },
      "source": [
        "\n",
        "## **4. Results**\n",
        "\n",
        "We plotted the input, output and the original images respectively, from a part of the dataset to find out the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmqceMH9FTnd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def display_results(generator, test_x, test_y, img_size=128, output_size=(1024, 1024), weights_path=\"/content/drive/MyDrive/Mestrado/2024_2/Topicos_especiais_IA_ThiagoPX/best_generator.weights.h5\"):\n",
        "\n",
        "    try:\n",
        "        generator.load_weights(weights_path)\n",
        "        print(f\"Pesos carregados com sucesso de: {weights_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar pesos: {e}\")\n",
        "        return\n",
        "\n",
        "    # Gerar previsões\n",
        "    y_pred = generator(test_x).numpy()\n",
        "\n",
        "    # Iterar sobre o conjunto de teste\n",
        "    for i, (input_img, target_img, output_img) in enumerate(zip(test_x, test_y, y_pred)):\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "        input_resized = Image.fromarray(input_img.reshape((128, 128)) * 255).resize(output_size)\n",
        "        axes[0].imshow(input_resized, cmap='gray')\n",
        "        axes[0].set_title(\"Imagem Cinza\", fontsize=12)\n",
        "        axes[0].axis(\"off\")\n",
        "\n",
        "        target_resized = Image.fromarray((target_img * 255).astype('uint8')).resize(output_size)\n",
        "        axes[1].imshow(target_resized)\n",
        "        axes[1].set_title(\"Saída Objetivo\", fontsize=12)\n",
        "        axes[1].axis(\"off\")\n",
        "\n",
        "        output_resized = Image.fromarray((output_img * 255).astype('uint8')).resize(output_size)\n",
        "        axes[2].imshow(output_resized)\n",
        "        axes[2].set_title(\"Imagem Colorida NN\", fontsize=12)\n",
        "        axes[2].axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "display_results(generator, test_x, test_y, img_size=256, output_size=(1024, 1024), weights_path=\"best_generator.weights.h5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lrvJmO4RFnL"
      },
      "source": [
        "Therefore, the overwhelming reslts depicts the power of GANs and the disruption which can be broght through them."
      ]
    }
  ]
}