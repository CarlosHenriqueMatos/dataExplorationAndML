{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10211,"databundleVersionId":111096,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/carlosmatos97/housepriceswithgraphs?scriptVersionId=213924779\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:27:29.213383Z","iopub.execute_input":"2024-12-19T21:27:29.213801Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/home-data-for-ml-course/sample_submission.csv\n/kaggle/input/home-data-for-ml-course/sample_submission.csv.gz\n/kaggle/input/home-data-for-ml-course/train.csv.gz\n/kaggle/input/home-data-for-ml-course/data_description.txt\n/kaggle/input/home-data-for-ml-course/test.csv.gz\n/kaggle/input/home-data-for-ml-course/train.csv\n/kaggle/input/home-data-for-ml-course/test.csv\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import pandas\nimport numpy\nimport matplotlib.pyplot as plt\nimport seaborn\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.execute_input":"2024-12-19T21:27:29.225744Z","iopub.status.idle":"2024-12-19T21:27:29.234781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def percentColumnNullValues(data_set, columns):\n    nan_columns = []\n    for column in columns:\n        if (data_set[column].isnull().sum() / data_set[column].shape[0] )*100 > 0:\n            print(column)\n            print((data_set[column].isnull().sum() / data_set[column].shape[0] )*100)\n            if column not in nan_columns:\n                nan_columns.append(column)\n    return nan_columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:27:29.241795Z","iopub.execute_input":"2024-12-19T21:27:29.242256Z","iopub.status.idle":"2024-12-19T21:27:29.250208Z","shell.execute_reply.started":"2024-12-19T21:27:29.242217Z","shell.execute_reply":"2024-12-19T21:27:29.249169Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"def nanValues(data_set,missing_values_columns_count):\n    total_cels = numpy.product(data_set.shape)\n    total_missing = missing_values_columns_count.sum()\n    percent_values_missing = (total_missing/total_cels)*100\n    print(percent_values_missing)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:27:29.251555Z","iopub.execute_input":"2024-12-19T21:27:29.251908Z","iopub.status.idle":"2024-12-19T21:27:29.267152Z","shell.execute_reply.started":"2024-12-19T21:27:29.251875Z","shell.execute_reply":"2024-12-19T21:27:29.26603Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"train_data_directory = '/kaggle/input/home-data-for-ml-course/train.csv'\ntest_data_directory = '/kaggle/input/home-data-for-ml-course/test.csv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:27:29.268814Z","iopub.execute_input":"2024-12-19T21:27:29.269188Z","iopub.status.idle":"2024-12-19T21:27:29.284905Z","shell.execute_reply.started":"2024-12-19T21:27:29.269162Z","shell.execute_reply":"2024-12-19T21:27:29.28386Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"train_dataset = pandas.read_csv(train_data_directory)\ntest_dataset = pandas.read_csv(test_data_directory)\ntrain_dataset.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ids = test_dataset.Id\nids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset.Id","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_train_values_columns_count = train_dataset.isnull().sum()\nnanValues(train_dataset, missing_train_values_columns_count)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_test_values_columns_count = test_dataset.isnull().sum()\nnanValues(test_dataset, missing_test_values_columns_count)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_train_values_columns_count","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_test_values_columns_count","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset_columns = train_dataset.columns\ntest_dataset_columns = test_dataset.columns\nprint(\"\\nTrain Dataset\\n\",train_dataset_columns);\nprint(\"\\nTest Dataset\\n\",test_dataset_columns);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset.dtypes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_nan_columns = percentColumnNullValues(train_dataset, train_dataset_columns)\ntest_nan_columns = percentColumnNullValues(test_dataset, test_dataset_columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#fazer um função para este codigo\nfor i in range(len(train_nan_columns)):\n    fig = px.histogram(train_dataset, x = train_nan_columns[i])\n    fig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#fazer um função para este codigo\nfor i in range(len(test_nan_columns)):\n    fig = px.histogram(test_dataset, x = test_nan_columns[i])\n    fig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def HousesSplitedByFiveYears(amountHousesPerYear):\n    quantity = []\n    year = 1900\n    for year in range(1900,2021,5):\n        amount=(amountHousesPerYear < year).sum()\n        quantity.append((year,amount))\n    yearAmount = pandas.DataFrame(numpy.array(quantity).reshape(25,2))\n    fig = go.Figure([go.Bar(x=yearAmount[0], y = yearAmount[1].values)])\n    fig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_amount_houses_per_year = train_dataset.YearBuilt\ntrain_year_amount = HousesSplitedByFiveYears(train_amount_houses_per_year)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_amount_houses_per_year = test_dataset.YearBuilt\ntest_year_amount = HousesSplitedByFiveYears(test_amount_houses_per_year)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for column in train_dataset_columns:\n    print(\"\\nThree exemples\")\n    print(train_dataset[column].head(3))\n    print(\"\\nValue counts\")\n    print(train_dataset[column].value_counts())\n    print(\"\\nColumn Description\")\n    print(train_dataset[column].describe(), end=\"\\n\\n\")\n    print(\"*\"*30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset['SalePrice'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seaborn.distplot(train_dataset['SalePrice']);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Skewness: %f\" % train_dataset[\"SalePrice\"].skew())\nprint(\"Kurtosis: %f\" % train_dataset[\"SalePrice\"].kurt())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"var = \"GrLivArea\"\ndata = pandas.concat([train_dataset[\"SalePrice\"], train_dataset[var]], axis = 1)\ndata.plot.scatter(x = var, y = 'SalePrice', ylim = (0, 800000));","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"var = \"TotalBsmtSF\"\ndata = pandas.concat([train_dataset[\"SalePrice\"], train_dataset[var]], axis = 1)\ndata.plot.scatter(x = var, y = 'SalePrice', ylim = (0, 800000));","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"var = \"OverallQual\"\ndata = pandas.concat([train_dataset[\"SalePrice\"], train_dataset[var]], axis = 1)\nf, ax = plt.subplots(figsize = (8, 6))\nfig = seaborn.boxplot(x = var, y = \"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"var = \"YearBuilt\"\ndata = pandas.concat([train_dataset[\"SalePrice\"], train_dataset[var]], axis = 1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = seaborn.boxplot(x = var, y = 'SalePrice', data = data)\nfig.axis(ymin = 0, ymax = 800000);\nplt.xticks(rotation = 90);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\ncategorical_columns = train_dataset.select_dtypes(include=['object']).columns\nencoder = OrdinalEncoder()\ntrain_dataset[categorical_columns] = encoder.fit_transform(train_dataset[categorical_columns])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"correlation_matrix = train_dataset.corr()\nf, ax = plt.subplots(figsize = (12, 9))\nseaborn.heatmap(correlation_matrix, vmax = .8, square = True);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming 'train_dataset' is your DataFrame\ncorrelation_matrix = train_dataset.corr()\n\n# Now you can get the top 'variables' correlations with \"SalePrice\"\nvariables = 10\ncols = correlation_matrix.nlargest(variables, \"SalePrice\")['SalePrice'].index\n\nimport numpy as np\nimport seaborn as sns\n\n# Compute the correlation matrix for the selected columns\nsalePrice_correlation = np.corrcoef(train_dataset[cols].values.T)\n\n# Create the heatmap\nsns.set(font_scale=1.25)\nhm = sns.heatmap(salePrice_correlation, cbar=True, annot=True, cmap=\"coolwarm\", xticklabels=cols, yticklabels=cols)\n\n# Display the heatmap\nhm.set_title(\"Correlation Heatmap of Variables with 'SalePrice'\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seaborn.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nseaborn.pairplot(train_dataset[cols], size = 2.5)\nplt.show();","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"totaldata = train_dataset.isnull().sum().sort_values(ascending = False)\npercent_null = (train_dataset.isnull().sum()/train_dataset.isnull().count()).sort_values(ascending = False)\nmissing_data = pandas.concat([totaldata, percent_null], axis = 1, keys=['Total', 'Percent'])\nmissing_data.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(missing_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"totaldata = train_dataset.isnull().sum().sort_values(ascending = False)\npercent_null = (train_dataset.isnull().sum()/train_dataset.isnull().count()).sort_values(ascending = False)\nmissing_data = pandas.concat([totaldata, percent_null], axis = 1, keys=['Total', 'Percent'])\nmissing_data.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"var = 'GrLivArea'\ndata = pandas.concat([train_dataset['SalePrice'], train_dataset[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), c = 'DarkBlue');","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seaborn.distplot(train_dataset['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train_dataset['SalePrice'], plot=plt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Log transformation\ntrain_dataset[\"SalePrice\"] = numpy.log(train_dataset['SalePrice'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#transformed histogram and normal probability plot\nseaborn.distplot(train_dataset['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train_dataset['SalePrice'], plot=plt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seaborn.distplot(train_dataset['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train_dataset['GrLivArea'], plot=plt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset['GrLivArea'] = numpy.log(train_dataset['GrLivArea'])\n#transformed histogram and normal probability plot\nseaborn.distplot(train_dataset['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train_dataset['GrLivArea'], plot=plt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seaborn.distplot(train_dataset['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train_dataset['TotalBsmtSF'], plot=plt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#create column for new variable (one is enough because it's a binary categorical feature)\n#if area>0 it gets 1, for area==0 it gets 0\ntrain_dataset['HasBsmt'] = pandas.Series(len(train_dataset['TotalBsmtSF']), index=train_dataset.index)\ntrain_dataset['HasBsmt'] = 0 \ntrain_dataset.loc[train_dataset['TotalBsmtSF']>0,'HasBsmt'] = 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset .loc[train_dataset['HasBsmt']==1,'TotalBsmtSF'] = numpy.log(train_dataset['TotalBsmtSF'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#histogram and normal probability plot\nseaborn.distplot(train_dataset[train_dataset['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train_dataset[train_dataset['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#scatter plot\nplt.scatter(train_dataset['GrLivArea'], train_dataset['SalePrice']);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nmodel_GBR =  GradientBoostingRegressor(n_estimators=1100, loss='squared_error', subsample = 0.35, learning_rate = 0.001,random_state=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef tratar_normalizar_dados(df):\n    # Copia os dados para evitar modificacoes indesejadas no original\n    df = df.copy()\n\n    # Preenche valores nulos para colunas numéricas com a média\n    for col in df.select_dtypes(include=['number']).columns:\n        df[col] = df[col].fillna(df[col].mean())\n\n    # Preenche valores ausentes para colunas categóricas com a moda\n    for col in df.select_dtypes(include=['object']).columns:\n        df[col] = df[col].fillna(df[col].mode()[0])\n\n    # Normaliza os dados numéricos para o intervalo [0, 1]\n    scaler = MinMaxScaler()\n    cols_numericas = df.select_dtypes(include=['number']).columns\n    df[cols_numericas] = scaler.fit_transform(df[cols_numericas])\n\n    return df\n\ndef atribuir_zero_nan(df):\n    # Substitui todos os valores NaN por 0\n    return df.fillna(0)\n\ndef ajustar_variaveis_categoricas(df):\n    # Converte variáveis categóricas em variáveis dummy\n    return pd.get_dummies(df, drop_first=True)\n\ndef alinhar_colunas(df1, df2):\n    # Obtém todas as colunas únicas dos dois DataFrames\n    todas_colunas = list(set(df1.columns).union(set(df2.columns)))\n\n    # Adiciona as colunas que faltam em cada DataFrame com valores 0\n    for col in todas_colunas:\n        if col not in df1.columns:\n            df1[col] = 0\n        if col not in df2.columns:\n            df2[col] = 0\n\n    # Reordena as colunas para que fiquem na mesma ordem\n    df1 = df1[todas_colunas]\n    df2 = df2[todas_colunas]\n\n    return df1, df2\n\n\ntrain_dataset = tratar_normalizar_dados(train_dataset)\ntrain_dataset = atribuir_zero_nan(train_dataset)\ntrain_dataset = ajustar_variaveis_categoricas(train_dataset)\n\ntest_dataset = tratar_normalizar_dados(test_dataset)\ntest_dataset = atribuir_zero_nan(test_dataset)\ntest_dataset = ajustar_variaveis_categoricas(test_dataset)\n\ntrain_dataset, test_dataset = alinhar_colunas(train_dataset, test_dataset)\n\nX_train = tratar_normalizar_dados(train_dataset)\nX_test = tratar_normalizar_dados(test_dataset)\nprint(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train = X_train.SalePrice\nX_train = X_train.drop(['Id','SalePrice'],axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test = X_test.drop(['Id','SalePrice'],axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def atribuirZeroNan(df):\n    # Substitui todos os valores NaN por 0\n    return df.fillna(0)\nX_train = atribuirZeroNan(X_train)\nX_test = atribuirZeroNan(X_test)\n\nmodel_GBR.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds_GBR = model_GBR.predict(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions=np.exp(preds_GBR)\nsubmission= pd.DataFrame({'Id': ids,'SalePrice': predictions})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train['SalePrice']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}