{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660ce795-9307-4c2c-98a1-beabcb36c740",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-0/basics.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/56295530-getting-set-up-video-guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
   "metadata": {},
   "source": [
    "# LangChain Academy\n",
    "\n",
    "Welcome to LangChain Academy! \n",
    "\n",
    "## Context\n",
    "\n",
    "At LangChain, we aim to make it easy to build LLM applications. One type of LLM application you can build is an agent. There’s a lot of excitement around building agents because they can automate a wide range of tasks that were previously impossible. \n",
    "\n",
    "In practice though, it is incredibly difficult to build systems that reliably execute on these tasks. As we’ve worked with our users to put agents into production, we’ve learned that more control is often necessary. You might need an agent to always call a specific tool first or use different prompts based on its state. \n",
    "\n",
    "To tackle this problem, we’ve built [LangGraph](https://langchain-ai.github.io/langgraph/) — a framework for building agent and multi-agent applications. Separate from the LangChain package, LangGraph’s core design philosophy is to help developers add better precision and control into agent workflows, suitable for the complexity of real-world systems.\n",
    "\n",
    "## Course Structure\n",
    "\n",
    "The course is structured as a set of modules, with each module focused on a particular theme related to LangGraph. You will see a folder for each module, which contains a series of notebooks. A video will accompany each notebook to help walk through the concepts, but the notebooks are also stand-alone, meaning that they contain explanations and can be viewed independently of the videos. Each module folder also contains a `studio` folder, which contains a set of graphs that can be loaded into [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio), our IDE for building LangGraph applications.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Before you begin, please follow the instructions in the `README` to create an environment and install dependencies.\n",
    "\n",
    "## Chat models\n",
    "\n",
    "In this course, we'll be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), which do a few things take a sequence of messages as inputs and return chat messages as outputs. LangChain does not host any Chat Models, rather we rely on third party integrations. [Here](https://python.langchain.com/v0.2/docs/integrations/chat/) is a list of 3rd party chat model integrations within LangChain! By default, the course will use [ChatOpenAI](https://python.langchain.com/v0.2/docs/integrations/chat/openai/) because it is both popular and performant. As noted, please ensure that you have an `OPENAI_API_KEY`.\n",
    "\n",
    "Let's check that your `OPENAI_API_KEY` is set and, if not, you will be asked to enter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9a52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_openai langchain_core langchain_community tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a15227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "#_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326f35b",
   "metadata": {},
   "source": [
    "[Here](https://python.langchain.com/v0.2/docs/how_to/#chat-models) is a useful how-to for all the things that you can do with chat models, but we'll show a few highlights below. If you've run `pip install -r requirements.txt` as noted in the README, then you've installed the `langchain-openai` package. With this, we can instantiate our `ChatOpenAI` model object. If you are signing up for the API for the first time, you should receive [free credits](https://community.openai.com/t/understanding-api-limits-and-free-tier/498517) that can be applied to any of the models. You can see pricing for various models [here](https://openai.com/api/pricing/). The notebooks will default to `gpt-4o` because it's a good balance of quality, price, and speed [see more here](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini), but you can also opt for the lower priced `gpt-3.5` series models. \n",
    "\n",
    "There are [a few standard parameters](https://python.langchain.com/v0.2/docs/concepts/#chat-models) that we can set with chat models. Two of the most common are:\n",
    "\n",
    "* `model`: the name of the model\n",
    "* `temperature`: the sampling temperature\n",
    "\n",
    "`Temperature` controls the randomness or creativity of the model's output where low temperature (close to 0) is more deterministic and focused outputs. This is good for tasks requiring accuracy or factual responses. High temperature (close to 1) is good for creative tasks or generating varied responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19a54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "#gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "#gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28450d1b",
   "metadata": {},
   "source": [
    "Chat models in LangChain have a number of [default methods](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). For the most part, we'll be using:\n",
    "\n",
    "* `stream`: stream back chunks of the response\n",
    "* `invoke`: call the chain on an input\n",
    "\n",
    "And, as mentioned, chat models take [messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as input. Messages have a role (that describes who is saying the message) and a content property. We'll be talking a lot more about this later, but here let's just show the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1280e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Lance\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "#gpt35_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac73e4c",
   "metadata": {},
   "source": [
    "We get an `AIMessage` response. Also, note that we can just invoke a chat model with a string. When a string is passed in as input, it is converted to a `HumanMessage` and then passed to the underlying model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e9cd113-c941-4b71-bd64-6f6a157850ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10538/1295426201.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robot: Beep boop! Hello human! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(\n",
    "    model=\"llama3\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d70f947-fa7a-4c80-8f50-a00bd1c5d673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(\n",
    "    model=\"qwen2.5:14b\", \n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbf705a0-0dee-4f66-8f84-83e22e7c0e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量子计算是一种利用量子力学原理进行信息处理和计算的技术。与传统计算机使用二进制位（比特，0或1）不同，量子计算机使用的是量子位（量子比特，简称“qubit”）。量子比特具有几个独特的性质：\n",
      "\n",
      "1. **叠加态**：一个经典比特在任何时刻只能是0或者1；而一个量子比特可以同时处于多种状态的组合中。例如，它可以同时表示0和1。\n",
      "\n",
      "2. **纠缠态**：两个或多个量子位之间可以通过一种特殊的方式相互关联，即使它们相隔很远，改变其中一个的状态会瞬间影响到另一个的状态。\n",
      "\n",
      "3. **干涉与测量**：量子系统中的粒子行为受到波粒二象性的影响，在特定条件下可以利用这种性质通过干涉来增强某些状态的概率，或者抑制其他状态的存在。当对量子位进行测量时，其叠加态会被“塌缩”为一个确定的值（0或1），并且这一过程是随机但概率性的。\n",
      "\n",
      "因为这些特性，量子计算机理论上可以在处理某些类型的问题上比传统计算机更加高效，尤其是在大规模数据搜索、密码破解以及模拟复杂物理系统等领域。然而，当前的技术还处于初级阶段，构建可靠且实用的大规模量子计算机仍然面临许多挑战。\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(\n",
    "    model=\"qwen2.5:14b\", \n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(\"用简单的术语解释量子计算\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "040e6c73-e7cc-4189-96b5-c85d76407588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Como assistente inteligente, eu não tenho capacidade de armazenar informações pessoais ou específicas sobre usuários entre sessões para respeitar a privacidade e segurança dos dados. No entanto, durante uma única sessão, posso lembrar da conversa anterior e continuar o diálogo com base nela. Se você precisar guardar alguma informação específica por um tempo mais longo, seria melhor anotá-la em algum lugar seguro ou usar um serviço de armazenamento de dados apropriado.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(\n",
    "    model=\"qwen2.5:14b\", \n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(\"voce consegue guardar informações?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bafd7d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro! Aqui está uma pequena surpresa:\n",
      "\n",
      "**Fato Curioso:** Você sabia que o maior número primo conhecido atualmente tem mais de 24 milhões de dígitos? Esse número foi encontrado através do projeto Great Internet Mersenne Prime Search (GIMPS), um esforço colaborativo global onde pessoas como você podem usar seus computadores para encontrar novos números primos grandes. Impressionante, não é?\n",
      "\n",
      "Espero que isso tenha te surpreendido! Se quiser mais informações ou outro tipo de surpresa, basta me dizer.\n",
      "Claro! Vamos fazer algo um pouco diferente e divertido. Imagine que você é um personagem em seu próprio conto de fadas moderno. Qual seria o seu superpoder neste mundo mágico, e como você usaria este poder para ajudar alguém ou mudar a sua realidade?\n",
      "Claro! Vamos fazer algo um pouco diferente e divertido. Imagine que você está em uma ilha tropical paradisíaca. Sua tarefa é criar o cocktail perfeito usando os ingredientes locais disponíveis, como frutas tropicais, hortelã-pimenta fresca e sucos naturais. Que tipo de cocktail criativo você gostaria de inventar? Ou se preferir, posso descrever um cocktail tropical delicioso para que você possa imaginá-lo enquanto eu conto!\n",
      "Claro! Aqui está uma pequena surpresa:\n",
      "\n",
      "**Fato interessante:** Você sabia que o maior número primo conhecido até hoje tem mais de 24 milhões de dígitos? Esse número foi descoberto em 2018 como parte do projeto de matemática colaborativa chamado \"Great Internet Mersenne Prime Search\" (GIMPS).\n",
      "\n",
      "Espero que isso tenha sido uma surpresa divertida! Tem algo mais com o qual posso ajudar?\n",
      "Claro! Vou fazer uma pequena brincadeira para te surpreender. Feche os olhos e cruze os dedos atrás da cabeça por 5 segundos. Agora, abra-os!\n",
      "\n",
      "Sério agora, você gostaria de alguma coisa específica? Uma piada, um fato interessante ou talvez algum conselho sobre algo que está preocupado?\n",
      "Claro! Vamos fazer algo um pouco diferente e divertido. Imagine que você é o criador de uma nova festa temática. Qual seria o tema dessa festa e por quê? Eu adoraria ouvir suas ideias criativas!\n",
      "Claro! Aqui está uma pequena surpresa para você:\n",
      "\n",
      "**Fato interessante:** Você sabia que os pássaros-campeões (champion shrew em inglês, mas geralmente não existe essa tradução direta; pode ser sobre o camundongo-campeão) são conhecidos por seu comportamento incomum de saltar e girar no ar quando excitados. No entanto, se você quis dizer um fato interessante sobre pássaros ou animais em geral, os pinguins podem correr com uma velocidade impressionante até 27 km/h quando precisam atravessar terras áridas para chegar ao mar.\n",
      "\n",
      "Se este não foi o tipo de surpresa que você estava procurando, por favor forneça mais detalhes!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMe surpreenda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/langchain_core/language_models/llms.py:387\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    378\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     **kwargs: Any,\n\u001b[32m    384\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    385\u001b[39m     config = ensure_config(config)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    398\u001b[39m         .text\n\u001b[32m    399\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/langchain_core/language_models/llms.py:764\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    757\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    761\u001b[39m     **kwargs: Any,\n\u001b[32m    762\u001b[39m ) -> LLMResult:\n\u001b[32m    763\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/langchain_core/language_models/llms.py:971\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    957\u001b[39m     run_managers = [\n\u001b[32m    958\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    959\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    969\u001b[39m         )\n\u001b[32m    970\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m    979\u001b[39m     run_managers = [\n\u001b[32m    980\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m    981\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    988\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m    989\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/langchain_core/language_models/llms.py:790\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    780\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    781\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    786\u001b[39m     **kwargs: Any,\n\u001b[32m    787\u001b[39m ) -> LLMResult:\n\u001b[32m    788\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    789\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    794\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    797\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    798\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    799\u001b[39m         )\n\u001b[32m    800\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    801\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/langchain_community/llms/ollama.py:437\u001b[39m, in \u001b[36mOllama._generate\u001b[39m\u001b[34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m generations = []\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     final_chunk = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m     generations.append([final_chunk])\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/langchain_community/llms/ollama.py:349\u001b[39m, in \u001b[36m_OllamaCommon._stream_with_aggregation\u001b[39m\u001b[34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_stream_with_aggregation\u001b[39m(\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    342\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    346\u001b[39m     **kwargs: Any,\n\u001b[32m    347\u001b[39m ) -> GenerationChunk:\n\u001b[32m    348\u001b[39m     final_chunk: Optional[GenerationChunk] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_generate_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stream_response_to_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/langchain_community/llms/ollama.py:194\u001b[39m, in \u001b[36m_OllamaCommon._create_generate_stream\u001b[39m\u001b[34m(self, prompt, stop, images, **kwargs)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_generate_stream\u001b[39m(\n\u001b[32m    187\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    188\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    191\u001b[39m     **kwargs: Any,\n\u001b[32m    192\u001b[39m ) -> Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    193\u001b[39m     payload = {\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt, \u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m: images}\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_stream(\n\u001b[32m    195\u001b[39m         payload=payload,\n\u001b[32m    196\u001b[39m         stop=stop,\n\u001b[32m    197\u001b[39m         api_url=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.base_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/generate\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    198\u001b[39m         **kwargs,\n\u001b[32m    199\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/requests/models.py:869\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self, chunk_size, decode_unicode, delimiter)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[32m    861\u001b[39m \u001b[33;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[32m    862\u001b[39m \u001b[33;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[32m    863\u001b[39m \n\u001b[32m    864\u001b[39m \u001b[33;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    867\u001b[39m pending = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/requests/utils.py:572\u001b[39m, in \u001b[36mstream_decode_response_unicode\u001b[39m\u001b[34m(iterator, r)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    571\u001b[39m decoder = codecs.getincrementaldecoder(r.encoding)(errors=\u001b[33m\"\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrv\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrv\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/urllib3/response.py:1063\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1060\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/urllib3/response.py:1219\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1216\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1218\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1219\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1221\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/urllib3/response.py:1138\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m line = \u001b[38;5;28mself\u001b[39m._fp.fp.readline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1139\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    response = llm.invoke(\"Me surpreenda\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35331da2-1cfa-4cc1-ae73-ba3b71d5f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 文本（中文）\n",
      "近年来，随着深度学习技术的迅速发展，基于自编码器、循环神经网络等模型的应用变得日益广泛。然而，在处理相似性度量任务时，传统的深度学习模型往往难以有效利用结构化数据中的潜在关系。针对这一问题，“孪生神经网络”（Siamese Neural Networks）应运而生，并在诸多领域取得了显著的成果。本文旨在探讨这种新颖且高效的神经网络架构及其实际应用案例。\n",
      "\n",
      "“孪生神经网络”的核心在于比较两个输入数据，通过对比学习的方式提取特征表示，并以此来判断输入数据之间的相似性。具体而言，同一对输入数据会同时被送入两个具有相同结构的神经网络中进行处理，输出的结果将直接用于计算两者之间的距离或余弦相似度等指标。\n",
      "\n",
      "在实际应用中，“孪生神经网络”广泛应用于图像识别、自然语言处理等领域。比如，在人脸识别技术领域，通过比较不同人的面部特征图来判断其身份；而在推荐系统方面，则可以利用用户的历史行为数据和当前的浏览记录进行对比分析，从而为用户提供个性化的服务。\n",
      "\n",
      "此外，近年来“孪生神经网络”的研究也取得了许多进展。特别是在计算机视觉任务中，如目标检测、图像分类等场景下，“孪生神经网络”通过结合先进的卷积操作和深度学习技术，能够更加准确地捕捉到不同样本之间的细微差异。这不仅为相关领域的研究人员提供了新的工具和技术支持，也为实际应用带来了无限的可能性。\n",
      "\n",
      "### Tradução para o Português\n",
      "Nos últimos anos, com o rápido desenvolvimento da tecnologia de aprendizado profundo (deep learning), aplicações baseadas em autoencoders e redes neurais recorrentes se tornaram cada vez mais prevalentes. No entanto, ao lidar com tarefas de medida de similaridade, modelos tradicionais de deep learning frequentemente têm dificuldade em utilizar efetivamente as relações potenciais dentro de dados estruturados. Diante dessa situação, surgiram as \"redes neurais siamesas\" (Siamese Neural Networks), que obtiveram resultados notáveis em várias áreas. Este artigo visa explorar essa nova e eficiente arquitetura neural assim como exemplos práticos de aplicação.\n",
      "\n",
      "O núcleo das redes neurais siamesas está na comparação de dois conjuntos de dados de entrada, extraíndo representações de características através do aprendizado por contraste para determinar a similaridade entre os dados de entrada. De forma específica, o mesmo par de entradas é processado simultaneamente em duas redes neurais idênticas, com seus resultados sendo utilizados diretamente para calcular distâncias ou métricas de similaridade cosseno.\n",
      "\n",
      "Na prática, as redes neurais siamesas são amplamente aplicadas em áreas como reconhecimento de imagem e processamento de linguagem natural. Por exemplo, no campo da identificação facial, comparações entre diferentes perfis faciais são feitas para determinar a identidade; já nos sistemas de recomendação, os dados históricos de comportamento do usuário e registros atuais podem ser comparados para fornecer serviços personalizados.\n",
      "\n",
      "Além disso, pesquisas sobre redes neurais siamesas também avançaram significativamente nos últimos anos. Em tarefas específicas de visão computacional como detecção de objetos e classificação de imagens, a combinação de operações convolucionais avançadas com técnicas de aprendizado profundo permite capturar diferenças sutis entre diferentes amostras mais precisamente. Isso não apenas fornece novas ferramentas e suporte tecnológico para pesquisadores em áreas relevantes, mas também abre inúmeras possibilidades para aplicações práticas.\n",
      "\n",
      "### Código Python\n",
      "Aqui está um exemplo simples de uma rede neural siamesa implementada usando o TensorFlow/Keras:\n",
      "\n",
      "```python\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras import layers\n",
      "\n",
      "# Definindo a arquitetura da rede neural\n",
      "def create_siamese_network(input_shape):\n",
      "    input_a = layers.Input(shape=input_shape)\n",
      "    input_b = layers.Input(shape=input_shape)\n",
      "\n",
      "    # Rede Neural Compartilhada (Shared Neural Network)\n",
      "    base_network = tf.keras.Sequential([\n",
      "        layers.Conv2D(64, 10, activation='relu', input_shape=input_shape),\n",
      "        layers.MaxPooling2D(),\n",
      "        layers.Conv2D(128, 7, activation='relu'),\n",
      "        layers.MaxPooling2D(),\n",
      "        layers.Conv2D(128, 4, activation='relu'),\n",
      "        layers.MaxPooling2D(),\n",
      "        layers.Flatten(),\n",
      "        layers.Dense(128, activation='sigmoid')\n",
      "    ])\n",
      "\n",
      "    # Aplicando a rede base aos dois conjuntos de entrada\n",
      "    processed_a = base_network(input_a)\n",
      "    processed_b = base_network(input_b)\n",
      "\n",
      "    # Calculando a distância entre os vetores processados (L2 norma)\n",
      "    distance = tf.keras.layers.Lambda(lambda tensors: tf.reduce_sum(tf.square(tensors[0] - tensors[1]), axis=1, keepdims=True))([processed_a, processed_b])\n",
      "\n",
      "    model = tf.keras.Model(inputs=[input_a, input_b], outputs=distance)\n",
      "\n",
      "    return model\n",
      "\n",
      "# Exemplo de uso\n",
      "if __name__ == '__main__':\n",
      "    # Define a forma dos dados de entrada (por exemplo, imagens 105x105)\n",
      "    input_shape = (105, 105, 1)\n",
      "\n",
      "    siamese_model = create_siamese_network(input_shape=input_shape)\n",
      "    siamese_model.summary()\n",
      "```\n",
      "\n",
      "Este código cria uma arquitetura básica para redes neurais siamesas usando o TensorFlow e Keras. A rede compartilha a mesma estrutura (base_network) entre dois conjuntos de entradas, comparando os resultados processados para calcular similaridade com base em distância Euclidiana.\n",
      "\n",
      "Por favor observe que este código é um exemplo simples e deve ser adaptado conforme necessário para atender aos requisitos específicos do seu projeto.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"escreva um texto em mandarim e depois traduza para o portugues com 10 paragrafos sobre redes neurais siamesas e me de um codigo escreito em python\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "857b3f6b-359c-4377-8758-8f309e557f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aqui está a tradução para o português:\n",
      "\n",
      "O texto fornecido é um excelente exemplo de como implementar uma rede neural siamesa utilizando Python e bibliotecas populares como Keras, Scikit-learn e Pandas. No entanto, há alguns pontos importantes a considerar:\n",
      "\n",
      "1. **Definição da Rede Siamesa**: A estrutura atual não está completamente alinhada com a arquitetura típica de uma rede siamesa. Na verdade, redes siamesas geralmente têm duas ramificações idênticas que compartilham pesos para processar dois vetores de entrada separadamente e depois calculam um score de similaridade entre eles. O exemplo fornecido parece ser mais próximo de uma arquitetura sequencial simples para classificação textual.\n",
      "\n",
      "2. **Função de Perda**: A função de perda `binary_crossentropy` é comumente usada em problemas de classificação binária, mas não é necessariamente apropriada para redes siamesas que geralmente utilizam uma função de perda contrastiva ou tripeta, dependendo do problema específico.\n",
      "\n",
      "3. **ETL e Pré-processamento**: Embora o código apresente um bom exemplo de ETL (Extract-Transform-Load) básico e pré-processamento para texto, é importante lembrar que a preparação dos dados pode ser muito mais complexa em aplicações reais, dependendo do tipo de entrada. Por exemplo, ao lidar com pares de imagens ou textos em um contexto siamese, seria necessário garantir que os pares sejam corretamente formatados e organizados antes da alimentação ao modelo.\n",
      "\n",
      "Para uma implementação mais precisa de redes neurais siamesas, aqui está uma estrutura simplificada mostrando como você poderia configurar duas ramificações idênticas (usando Keras Functional API) que processam dois vetores de entrada diferentes e calculam a similaridade entre eles:\n",
      "\n",
      "```python\n",
      "from keras.layers import Lambda\n",
      "\n",
      "# Definindo o modelo base para um dos ramos da rede siamesa.\n",
      "def create_base_network(input_dim):\n",
      "    input_layer = Input(shape=(input_dim,))\n",
      "    x = Dense(64, activation='relu')(input_layer)\n",
      "    x = Dense(32, activation='relu')(x)\n",
      "    return Model(inputs=input_layer, outputs=x)\n",
      "\n",
      "# Criando os dois ramos idênticos para a rede siamesa.\n",
      "base_network = create_base_network(X_train.shape[1])\n",
      "encoded_a = base_network(input_layer)\n",
      "encoded_b = base_network(Input(shape=(X_train.shape[1],)))\n",
      "\n",
      "# Calculando similaridade\n",
      "distance = Lambda(euclidean_distance)([encoded_a, encoded_b])\n",
      "\n",
      "siamese_model = Model(inputs=[input_layer, Input(shape=(X_train.shape[1],))], outputs=distance)\n",
      "\n",
      "def euclidean_distance(vects):\n",
      "    x, y = vects\n",
      "    return K.sqrt(K.maximum(K.sum(K.pow(x - y, 2), axis=1, keepdims=True), K.epsilon()))\n",
      "\n",
      "# Compilação do modelo com uma função de perda apropriada (por exemplo, contrastive loss)\n",
      "siamese_model.compile(optimizer='adam', loss=contrastive_loss)\n",
      "\n",
      "def contrastive_loss(y_true, y_pred):\n",
      "    margin = 1\n",
      "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.maximum(margin - y_pred, 0))\n",
      "\n",
      "# Treinamento do modelo e avaliação\n",
      "siamese_model.fit([X_train, X_train], labels, batch_size=64, epochs=20)\n",
      "```\n",
      "\n",
      "Este código define a arquitetura básica de uma rede siamesa onde dois vetores de entrada (um par) são processados por duas ramificações idênticas e depois calculam a distância euclidiana entre eles como um score de similaridade. A função `contrastive_loss` é usada para treinar o modelo para distinguir pares semelhantes e não semelhantes.\n",
      "\n",
      "Lembre-se de que este exemplo é bastante simplificado e pode precisar ser ajustado dependendo dos requisitos específicos do seu problema, como a natureza exata da entrada (texto, imagem) e as necessidades de pré-processamento.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"escreva um prompt em mandarim para a contrução de um codigo em python sobre redes neurais siamesas eu quero o codigo completo com todo processo de etl e transformações dos dados se possivel teste o codigo não simplifique nada quero tudo bem descrito eu não quero implementar nenhum codigo quero somente rodar então me entregue o codigo todo pronto\")\n",
    "response = llm.invoke(response)\n",
    "response = llm.invoke(\"Traduza para o portugues: \"+response)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc11b0b-1bf8-4ef7-9110-98c7c36a5592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
