{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660ce795-9307-4c2c-98a1-beabcb36c740",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-0/basics.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/56295530-getting-set-up-video-guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
   "metadata": {},
   "source": [
    "# LangChain Academy\n",
    "\n",
    "Welcome to LangChain Academy! \n",
    "\n",
    "## Context\n",
    "\n",
    "At LangChain, we aim to make it easy to build LLM applications. One type of LLM application you can build is an agent. There’s a lot of excitement around building agents because they can automate a wide range of tasks that were previously impossible. \n",
    "\n",
    "In practice though, it is incredibly difficult to build systems that reliably execute on these tasks. As we’ve worked with our users to put agents into production, we’ve learned that more control is often necessary. You might need an agent to always call a specific tool first or use different prompts based on its state. \n",
    "\n",
    "To tackle this problem, we’ve built [LangGraph](https://langchain-ai.github.io/langgraph/) — a framework for building agent and multi-agent applications. Separate from the LangChain package, LangGraph’s core design philosophy is to help developers add better precision and control into agent workflows, suitable for the complexity of real-world systems.\n",
    "\n",
    "## Course Structure\n",
    "\n",
    "The course is structured as a set of modules, with each module focused on a particular theme related to LangGraph. You will see a folder for each module, which contains a series of notebooks. A video will accompany each notebook to help walk through the concepts, but the notebooks are also stand-alone, meaning that they contain explanations and can be viewed independently of the videos. Each module folder also contains a `studio` folder, which contains a set of graphs that can be loaded into [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio), our IDE for building LangGraph applications.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Before you begin, please follow the instructions in the `README` to create an environment and install dependencies.\n",
    "\n",
    "## Chat models\n",
    "\n",
    "In this course, we'll be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), which do a few things take a sequence of messages as inputs and return chat messages as outputs. LangChain does not host any Chat Models, rather we rely on third party integrations. [Here](https://python.langchain.com/v0.2/docs/integrations/chat/) is a list of 3rd party chat model integrations within LangChain! By default, the course will use [ChatOpenAI](https://python.langchain.com/v0.2/docs/integrations/chat/openai/) because it is both popular and performant. As noted, please ensure that you have an `OPENAI_API_KEY`.\n",
    "\n",
    "Let's check that your `OPENAI_API_KEY` is set and, if not, you will be asked to enter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc8b3f9-54d0-4203-9400-23dd90a3fa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Nenhuma GPU CUDA disponível\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f9a52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture --no-stderr\n",
    "#%pip install --quiet -U langchain_openai langchain_core langchain_community tavily-python\n",
    "from langchain_community.llms import Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a15227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "#_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326f35b",
   "metadata": {},
   "source": [
    "[Here](https://python.langchain.com/v0.2/docs/how_to/#chat-models) is a useful how-to for all the things that you can do with chat models, but we'll show a few highlights below. If you've run `pip install -r requirements.txt` as noted in the README, then you've installed the `langchain-openai` package. With this, we can instantiate our `ChatOpenAI` model object. If you are signing up for the API for the first time, you should receive [free credits](https://community.openai.com/t/understanding-api-limits-and-free-tier/498517) that can be applied to any of the models. You can see pricing for various models [here](https://openai.com/api/pricing/). The notebooks will default to `gpt-4o` because it's a good balance of quality, price, and speed [see more here](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini), but you can also opt for the lower priced `gpt-3.5` series models. \n",
    "\n",
    "There are [a few standard parameters](https://python.langchain.com/v0.2/docs/concepts/#chat-models) that we can set with chat models. Two of the most common are:\n",
    "\n",
    "* `model`: the name of the model\n",
    "* `temperature`: the sampling temperature\n",
    "\n",
    "`Temperature` controls the randomness or creativity of the model's output where low temperature (close to 0) is more deterministic and focused outputs. This is good for tasks requiring accuracy or factual responses. High temperature (close to 1) is good for creative tasks or generating varied responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19a54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "#gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "#gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28450d1b",
   "metadata": {},
   "source": [
    "Chat models in LangChain have a number of [default methods](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). For the most part, we'll be using:\n",
    "\n",
    "* `stream`: stream back chunks of the response\n",
    "* `invoke`: call the chain on an input\n",
    "\n",
    "And, as mentioned, chat models take [messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as input. Messages have a role (that describes who is saying the message) and a content property. We'll be talking a lot more about this later, but here let's just show the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1280e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Lance\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "#gpt35_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac73e4c",
   "metadata": {},
   "source": [
    "We get an `AIMessage` response. Also, note that we can just invoke a chat model with a string. When a string is passed in as input, it is converted to a `HumanMessage` and then passed to the underlying model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e9cd113-c941-4b71-bd64-6f6a157850ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3072/2360961892.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(\n",
    "    model=\"llama3\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.7\n",
    ")\n",
    "#response = llm.invoke(messages)\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d70f947-fa7a-4c80-8f50-a00bd1c5d673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(\n",
    "    model=\"qwen2.5:14b\", \n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=1.0\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a44ce16-8759-4395-ba2f-244fbaeb7c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4244/2094665964.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(\n",
    "    model=\"qwen2.5:14b\", \n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbf705a0-0dee-4f66-8f84-83e22e7c0e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50744/3411072553.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(\n",
    "    model=\"qwen2.5:14b\", \n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.7\n",
    ")\n",
    "#response = llm.invoke(\"用简单的术语解释量子计算\")\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "040e6c73-e7cc-4189-96b5-c85d76407588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Como assistente de inteligência artificial, eu não armazeno informações pessoais ou específicas sobre as conversas que tenho com os usuários. Cada interação é tratada independentemente e privacidade dos usuários é uma prioridade para mim. No entanto, durante nossa sessão atual, posso lembrar das informações relevantes necessárias para fornecer assistência eficaz. Se você tiver alguma informação específica que gostaria de manter em mente durante nossa conversa, por favor me informe e farei o meu melhor para ajudar com base nessa informação.\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(\n",
    "    model=\"qwen2.5:14b\", \n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.2\n",
    ")\n",
    "response = llm.invoke(\"voce consegue guardar informações?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fd17ec5-532f-47cf-b61c-5dda60766902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro, vou anotar essas informações para futuras consultas:\n",
      "\n",
      "- Certifique-se de instalar todas as bibliotecas necessárias antes de começar o desenvolvimento.\n",
      "- Verifique a compatibilidade da rede neural com os recursos disponíveis no seu hardware (GPU e CPU).\n",
      "- Para evitar problemas de capacidade de processamento, redimensione as imagens para 256x256 pixels.\n",
      "- Você possui uma GPU NVIDIA RTX 3060 com 12GB de VRAM.\n",
      "- Seu computador é um processador Intel Core i5 8600K com 6 núcleos (threads).\n",
      "- O sistema tem 32GB de memória RAM.\n",
      "\n",
      "Essas informações serão úteis para orientar o desenvolvimento e garantir que os recursos do seu hardware sejam utilizados da melhor maneira possível. Se precisar de ajuda específica com algum aspecto desses detalhes, estou à disposição!\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(\n",
    "    model=\"qwen2.5:14b\", \n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.2\n",
    ")\n",
    "response = llm.invoke(\"\"\"\n",
    "Preciso que voce guarde essas informações para desenvolvimento fututo:\n",
    "Para lembre-se de colocar o install de todas as bibliotecas que forem ser utilizadas,não se esqueça de verficar se a rede neural\n",
    "vai suportar, para evitar isso redimensione a imagem para 256x256\n",
    "eu possuo uma 3060 com 12gb de Gram e meu computador é um I5 8600k com 6threads meu computador possui 32gb de ram\n",
    "\"\"\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "857b3f6b-359c-4377-8758-8f309e557f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Este script Python implementa uma abordagem completa para treinar um modelo Siamese Network usando o dataset CIFAR-10 com PyTorch. Ele inclui várias etapas importantes:\n",
      "\n",
      "1. **Criação de Pares**: O script cria pares de imagens positivos e negativos a partir do conjunto de dados CIFAR-10, que é essencial para treinar uma rede Siamese.\n",
      "\n",
      "2. **Modelo Siamese Network**: Define um modelo Siamese simples com duas redes idênticas (embedding_net) que compartilham pesos. O modelo é capaz de receber dois tensores como entrada e retornar os embeddings correspondentes.\n",
      "\n",
      "3. **Contrastive Loss**: Implementa a função de perda contrastiva, que é crucial para o treinamento da rede Siamese. Esta função calcula a distância entre os embeddings dos pares de imagens e ajusta essa distância com base na similaridade (positivo ou negativo) do par.\n",
      "\n",
      "4. **Treino e Avaliação**: O script inclui funções para treinar e avaliar o modelo, usando validação cruzada estratificada (k=5). Isso ajuda a obter uma estimativa mais precisa da performance do modelo em dados não vistos.\n",
      "\n",
      "5. **Seleção de Hiperparâmetros**: Realiza um loop sobre diferentes configurações de hiperparâmetros para encontrar a melhor combinação que maximize a acurácia média durante o treinamento e validação cruzada.\n",
      "\n",
      "6. **Teste t Pareado**: Após identificar a melhor configuração, o script realiza um teste t pareado entre essa configuração e outras para verificar se as diferenças nas métricas de desempenho são estatisticamente significativas.\n",
      "\n",
      "### Considerações Importantes:\n",
      "\n",
      "- O código atualiza os pesos do modelo em cada fold da validação cruzada sem guardar modelos anteriores. Isso significa que o melhor conjunto de hiperparâmetros é determinado com base nas médias das métricas durante a validação cruzada, não nos melhores modelos individuais.\n",
      "\n",
      "- A função `create_siamese_pairs` pode gerar um grande número de pares, especialmente para conjuntos de dados grandes como CIFAR-10. Isso poderia ser otimizado ou limitado dependendo das necessidades computacionais.\n",
      "\n",
      "- O script atualmente não salva os melhores modelos durante o treinamento e validação cruzada; apenas registra as métricas. Se quiser salvar os pesos do modelo para uso posterior, você precisaria adicionar essa funcionalidade.\n",
      "\n",
      "### Melhorias Possíveis:\n",
      "\n",
      "1. **Aumento de Dados**: Adicionar técnicas como aumento de dados durante o treinamento pode melhorar a generalização do modelo.\n",
      "2. **Regularização**: Incluir regularização (como dropout) para evitar overfitting.\n",
      "3. **Visualização dos Resultados**: Adicionar visualizações das métricas ao longo do tempo e dos embeddings gerados pelo modelo podem fornecer insights adicionais sobre o desempenho do modelo.\n",
      "\n",
      "Este script é uma implementação robusta que pode ser facilmente adaptada para outros conjuntos de dados ou tarefas semelhantes.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"\"\"\n",
    "eu sei tambem que a geração completa do código Python diretamente aqui seria extensa e complexa demais para ser gerada em tempo real \n",
    "por este assistente ma quero que voce faça mesmo assim código completo em Python usando PyTorch para treinar uma rede neural siamesa\n",
    "para classificação de similaridade de imagens, com as seguintes exigências:\n",
    "1. Utilize o dataset CIFAR-10 do torchvision.\n",
    "2. Construa um dataset de **pares de imagens** (pares positivos e negativos) para treinar o modelo siamesa.\n",
    "3. Modele uma **rede siamesa simples**, com backbone CNN compartilhado e camada de embedding.\n",
    "4. Utilize **contrastive loss** como função de perda.\n",
    "5. Aplique **validação cruzada estratificada (k=5)** nos pares.\n",
    "6. Faça **seleção de hiperparâmetros** para:\n",
    "   - Taxa de aprendizado ([0.01, 0.001])\n",
    "   - Dimensão do embedding ([64, 128])\n",
    "   - Número de filtros na primeira camada ([16, 32])\n",
    "7. Para cada fold, calcule:\n",
    "   - Acurácia de predição de similaridade\n",
    "   - F1-Score (macro)\n",
    "8. Ao final dos folds:\n",
    "   - Calcule médias e desvios padrão das métricas\n",
    "   - Execute um **teste t de Student pareado** entre os dois melhores conjuntos de hiperparâmetros\n",
    "9. Gere uma **análise textual automática** dos resultados: qual configuração teve melhor desempenho, se a diferença é estatisticamente significativa etc.\n",
    "10. Use apenas PyTorch, torchvision, scikit-learn e scipy.\n",
    "11. Adicione comentários detalhados no código explicando cada parte.\n",
    "12. O código deve ser completo, executável e sem textos fora de #comentarios.\n",
    "\"\"\")\n",
    "response = llm.invoke(response)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9995effc-273d-416e-918d-8f510ceff982",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    model=\"qwen2.5:14b\", \n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c08147-bb0f-4d8b-97bb-18b1aebc4c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O código fornecido é bem organizado para realizar a tarefa de encontrar o melhor conjunto de hiperparâmetros para um modelo siamesa baseado no dataset CIFAR-10. No entanto, há algumas áreas que precisam ser melhoradas ou ajustadas para garantir sua funcionalidade total:\n",
      "\n",
      "### Correções e Melhorias\n",
      "\n",
      "1. **Carregamento dos Dados**: O código atualmente carrega os dados do CIFAR-10 mas não cria DataLoaders para eles. É necessário criar `train_loader` e `val_loader` que serão usados durante o treinamento e teste.\n",
      "\n",
      "2. **Criar Pares de Imagens**: A função `create_pairs()` está mal implementada. A versão atual gera uma quantidade arbitrária de pares, não considerando os índices do conjunto de dados e nem garantindo um equilíbrio entre classes diferentes (positivos e negativos).\n",
      "\n",
      "3. **Treinamento no KFold**: O código realiza a validação cruzada `StratifiedKFold` mas não separa o conjunto de treino em conjuntos de treino e validação para cada fold.\n",
      "\n",
      "4. **Comparação de Configurações com Teste t Pareado**: O cálculo do teste t pareado está correto, porém precisa ser ajustado a fim de garantir que todas as médias correspondam aos pares corretos das configurações durante o processo comparativo. Isso inclui filtragem correta e exclusão de `NaN` ao calcular médias.\n",
      "\n",
      "### Implementação Corrigida\n",
      "\n",
      "Vamos fazer os ajustes necessários no código para torná-lo funcional:\n",
      "\n",
      "1. **Carregamento dos Dados com DataLoaders**:\n",
      "   \n",
      "   ```python\n",
      "   train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
      "   test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
      "   ```\n",
      "\n",
      "2. **Criar Pares de Imagens Corretamente**:\n",
      "   \n",
      "   A função `create_pairs()` precisa ser ajustada para considerar os índices corretos do conjunto de dados e gerar um número apropriado de pares positivos e negativos.\n",
      "   \n",
      "3. **Validação Cruzada Estratificada (K-Fold)**:\n",
      "   \n",
      "   É necessário criar conjuntos de treino e validação separadamente para cada fold.\n",
      "\n",
      "4. **Análise das Configurações com Teste t Pareado**:\n",
      "   \n",
      "   Precisamos garantir que a comparação entre configurações seja feita corretamente, usando apenas os resultados relevantes sem introduzir `NaN`.\n",
      "\n",
      "### Exemplo de Correção Parcial\n",
      "\n",
      "Aqui está um exemplo de como corrigir uma das partes do código (criação dos loaders e correção da função create_pairs):\n",
      "\n",
      "```python\n",
      "from torch.utils.data import DataLoader\n",
      "import random\n",
      "\n",
      "def load_data():\n",
      "    transform = transforms.Compose([\n",
      "        transforms.ToTensor(),\n",
      "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
      "    ])\n",
      "\n",
      "    train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
      "    test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
      "\n",
      "    return DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True), DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
      "\n",
      "\n",
      "def create_pairs(data_loader, labels):\n",
      "    pairs = []\n",
      "    targets = []\n",
      "\n",
      "    class_indices = {}\n",
      "    \n",
      "    # Mapa de índices para classes\n",
      "    for idx, label in enumerate(labels):\n",
      "        if label not in class_indices:\n",
      "            class_indices[label] = []\n",
      "        class_indices[label].append(idx)\n",
      "        \n",
      "    for _ in range(int(len(data_loader.dataset)/data_loader.batch_size)):\n",
      "        alpha = random.choice(range(10))\n",
      "        beta = random.choice(range(10))\n",
      "\n",
      "        if alpha != beta:  # Par negativo\n",
      "            target = 0\n",
      "            image1_idx = random.choice(class_indices[alpha])\n",
      "            image2_idx = random.choice(class_indices[beta])\n",
      "            \n",
      "        else:\n",
      "            target = 1  # Par positivo\n",
      "            image1_idx = random.choice(class_indices[alpha])\n",
      "            image2_idx = random.choice([idx for idx in class_indices[alpha] if idx != image1_idx])\n",
      "\n",
      "        pairs.append((image1_idx, image2_idx))\n",
      "        targets.append(target)\n",
      "        \n",
      "    return torch.LongTensor(targets), [list(pair) for pair in pairs]\n",
      "```\n",
      "\n",
      "### Conclusão\n",
      "\n",
      "Com essas correções e melhorias, o código será capaz de carregar os dados corretamente, criar pares adequados de imagens para treinamento e validação cruzada estratificada. A análise estatística dos resultados garantirá que as diferenças nas métricas entre diferentes configurações de hiperparâmetros sejam significativas.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"\"\"\n",
    "eu sei tambem que a geração completa do código Python diretamente aqui seria extensa e complexa demais para ser gerada em tempo real \n",
    "por este assistente ma quero que voce faça mesmo assim código completo em Python usando PyTorch para treinar uma rede neural siamesa\n",
    "para classificação de similaridade de imagens, com as seguintes exigências:\n",
    "1. Utilize o dataset CIFAR-10 do torchvision.\n",
    "2. Construa um dataset de **pares de imagens** (pares positivos e negativos) para treinar o modelo siamesa.\n",
    "3. Modele uma **rede siamesa simples**, com backbone CNN compartilhado e camada de embedding.\n",
    "4. Utilize **contrastive loss** como função de perda.\n",
    "5. Aplique **validação cruzada estratificada (k=5)** nos pares.\n",
    "6. Faça **seleção de hiperparâmetros** para:\n",
    "   - Taxa de aprendizado ([0.01, 0.001])\n",
    "   - Dimensão do embedding ([64, 128])\n",
    "   - Número de filtros na primeira camada ([16, 32])\n",
    "7. Para cada fold, calcule:\n",
    "   - Acurácia de predição de similaridade\n",
    "   - F1-Score (macro)\n",
    "8. Ao final dos folds:\n",
    "   - Calcule médias e desvios padrão das métricas\n",
    "   - Execute um **teste t de Student pareado** entre os dois melhores conjuntos de hiperparâmetros\n",
    "9. Gere uma **análise textual automática** dos resultados: qual configuração teve melhor desempenho, se a diferença é estatisticamente significativa etc.\n",
    "10. Use apenas PyTorch, torchvision, scikit-learn e scipy.\n",
    "11. Adicione comentários detalhados no código explicando cada parte.\n",
    "12. O código deve ser completo, executável e sem textos fora de #comentarios.\n",
    "\"\"\")\n",
    "response = llm.invoke(response)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "805252ae-ea0d-4c23-bb1f-2d9964cd51e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'img2' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 154\u001b[39m\n\u001b[32m    151\u001b[39m     criterion = contrastive_loss\n\u001b[32m    152\u001b[39m     optimizer = optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[43mtrain_and_validate_siamese\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# Avaliação final no conjunto de teste\u001b[39;00m\n\u001b[32m    157\u001b[39m accuracy, f1_score_ = evaluate_siamese(model, test_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mtrain_and_validate_siamese\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, epochs)\u001b[39m\n\u001b[32m     81\u001b[39m model.train()\n\u001b[32m     82\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mSiameseDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     71\u001b[39m     img1 = \u001b[38;5;28mself\u001b[39m.transform(img1)\n\u001b[32m     72\u001b[39m     img2 = \u001b[38;5;28mself\u001b[39m.transform(img2)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (img1, \u001b[43mimg2\u001b[49m), \u001b[38;5;28mint\u001b[39m(similar_label)\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'img2' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import scipy.stats as st\n",
    "from PIL import Image\n",
    "\n",
    "# Transformação de dados\n",
    "transform = Resize((256, 256))\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=4096, n_filters=32):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # CNN backbone compartilhado\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, n_filters, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(n_filters, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        # Camada de embedding\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(1024, embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        output = self.cnn(x)\n",
    "        output = output.view(output.size()[0], -1)  # Flattening\n",
    "        output = self.embedding(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        return self.forward_one(input1), self.forward_one(input2)\n",
    "\n",
    "def contrastive_loss(embedding_a, embedding_b, label, margin=1.0):\n",
    "    distance = torch.pairwise_distance(embedding_a, embedding_b)\n",
    "\n",
    "    loss_contrastive = torch.mean((label) * torch.pow(distance, 2) +\n",
    "                                    (1 - label) * torch.pow(torch.clamp(margin - distance, min=0.0), 2))\n",
    "    return loss_contrastive\n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        self.mnist = mnist_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1, label1 = self.mnist[index]\n",
    "\n",
    "        # Seleciona uma segunda imagem com a mesma classe ou diferente de acordo com o rótulo desejado.\n",
    "        if torch.rand(1) > 0.5:\n",
    "            similar_label = True\n",
    "            label2 = label1\n",
    "        else:\n",
    "            similar_label = False\n",
    "            while True:\n",
    "                index2 = torch.randint(len(self.mnist), (1,))\n",
    "                img2, label2 = self.mnist[index2]\n",
    "                if label2 != label1: break\n",
    "\n",
    "        # Transformação de imagens para 256x256\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return (img1, img2), int(similar_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "\n",
    "def train_and_validate_siamese(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for (img1, img2), label in dataloader:\n",
    "            img1, img2, label = img1.to(device).unsqueeze(1), img2.to(device).unsqueeze(1), torch.tensor([label]).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            embedding_a, embedding_b = model(img1.float(), img2.float())\n",
    "            loss = criterion(embedding_a, embedding_b, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * len(label)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs} - Loss: {running_loss / len(dataloader.dataset):.4f}')\n",
    "\n",
    "def evaluate_siamese(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (img1, img2), label in dataloader:\n",
    "            img1, img2, label = img1.to(device).unsqueeze(1), img2.to(device).unsqueeze(1), torch.tensor([label]).to(device)\n",
    "\n",
    "            embedding_a, embedding_b = model(img1.float(), img2.float())\n",
    "            distance = torch.pairwise_distance(embedding_a, embedding_b)\n",
    "\n",
    "            pred = (distance < 0.5).int().cpu()\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_preds.extend(pred.numpy())\n",
    "\n",
    "    return accuracy_score(all_labels, all_preds), f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "# Configuração do dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Carregamento dos dados MNIST\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Redimensionamento das imagens para 256x256\n",
    "train_dataset.transform = transform\n",
    "test_dataset.transform = transform\n",
    "\n",
    "# Criação do dataset personalizado e carregamento em DataLoader\n",
    "siamese_train_dataset = SiameseDataset(train_dataset)\n",
    "siamese_test_dataset = SiameseDataset(test_dataset)\n",
    "\n",
    "train_loader = DataLoader(siamese_train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(siamese_test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Inicialização do modelo e otimizador\n",
    "model = SiameseNetwork().to(device)\n",
    "criterion = contrastive_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Treinamento e validação com k-fold cross-validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(siamese_train_dataset)):\n",
    "    print(f'Fold {fold + 1}/{k_folds}')\n",
    "\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_loader_fold = DataLoader(siamese_train_dataset, batch_size=64, sampler=train_sampler)\n",
    "    val_loader_fold = DataLoader(siamese_train_dataset, batch_size=64, sampler=valid_sampler)\n",
    "\n",
    "    model = SiameseNetwork().to(device)  # Reinicializa o modelo para cada fold\n",
    "    criterion = contrastive_loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_and_validate_siamese(model, train_loader_fold, criterion, optimizer, epochs=5)\n",
    "\n",
    "# Avaliação final no conjunto de teste\n",
    "accuracy, f1_score_ = evaluate_siamese(model, test_loader)\n",
    "print(f'Test Accuracy: {accuracy:.4f}, F1 Score: {f1_score_:.4f}')\n",
    "\n",
    "# Análise dos resultados e comparação de hiperparâmetros (exemplo simplificado)\n",
    "best_model_params = {'lr': 0.001, 'epochs': 5}\n",
    "second_best_model_params = {'lr': 0.0001, 'epochs': 10}\n",
    "\n",
    "# Supondo que os resultados foram armazenados\n",
    "results_best_model = (accuracy, f1_score_)\n",
    "results_second_best_model = (accuracy - 0.02, f1_score_ - 0.01)\n",
    "\n",
    "statistic, p_value = st.ttest_ind([results_best_model[1]], [results_second_best_model[1]])\n",
    "print(f'p-value: {p_value}')\n",
    "if p_value < 0.05:\n",
    "    print('There is a statistically significant difference between the models.')\n",
    "else:\n",
    "    print('No statistically significant difference found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3551ea35-13fa-4afd-94a9-fc1c5d1ab497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error you encountered (`NameError: name 'distance' not defined`) in your `evaluate_siamese` function indicates that there's an issue with how the distance calculation and prediction logic are structured. Specifically, it seems like the variable `distance` is being used before it is properly computed.\n",
      "\n",
      "Let's address this by ensuring that the distance computation and prediction steps are correctly implemented:\n",
      "\n",
      "1. **Ensure Distance Calculation**: The pairwise distance between embeddings should be calculated.\n",
      "2. **Threshold Prediction**: Use a threshold (e.g., 0.5) to convert distances into binary predictions.\n",
      "3. **Collect Metrics**: Collect all labels and predictions for evaluation.\n",
      "\n",
      "Here's the corrected `evaluate_siamese` function:\n",
      "\n",
      "```python\n",
      "def evaluate_siamese(model, dataloader):\n",
      "    model.eval()\n",
      "    all_labels = []\n",
      "    all_preds = []\n",
      "\n",
      "    with torch.no_grad():\n",
      "        for (img1, img2), label in dataloader:\n",
      "            img1, img2, label = img1.to(device).unsqueeze(1), img2.to(device).unsqueeze(1), torch.tensor([label]).to(device)\n",
      "\n",
      "            embedding_a, embedding_b = model(img1.float(), img2.float())\n",
      "            distance = torch.pairwise_distance(embedding_a, embedding_b)  # Ensure this line is correct\n",
      "\n",
      "            pred = (distance < 0.5).int().cpu()  # Convert distances to binary predictions\n",
      "            all_labels.extend(label.cpu().numpy())  # Collect labels\n",
      "            all_preds.extend(pred.numpy())  # Collect predictions\n",
      "\n",
      "    return accuracy_score(all_labels, all_preds), f1_score(all_labels, all_preds, average='macro')\n",
      "```\n",
      "\n",
      "### Full Code with Fixes:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "from torchvision import datasets, transforms\n",
      "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.metrics import accuracy_score, f1_score\n",
      "\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "# Data loading and preprocessing\n",
      "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
      "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
      "\n",
      "# Custom dataset for Siamese network\n",
      "class SiameseDataset:\n",
      "    def __init__(self, dataset):\n",
      "        self.dataset = dataset\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        img1, label1 = self.dataset[idx]\n",
      "        while True:\n",
      "            img2, label2 = self.dataset[torch.randint(len(self.dataset), (1,)).item()]\n",
      "            if label1 != label2:  # Ensure different labels\n",
      "                break\n",
      "        return img1, img2, int(label1 == label2)\n",
      "\n",
      "# Custom dataset for Siamese network with resizing\n",
      "train_dataset.transform = transforms.Compose([transforms.Resize((256, 256)), train_dataset.transform])\n",
      "test_dataset.transform = transforms.Compose([transforms.Resize((256, 256)), test_dataset.transform])\n",
      "\n",
      "siamese_train_dataset = SiameseDataset(train_dataset)\n",
      "siamese_test_dataset = SiameseDataset(test_dataset)\n",
      "\n",
      "train_loader = DataLoader(siamese_train_dataset, batch_size=64, shuffle=True)\n",
      "test_loader = DataLoader(siamese_test_dataset, batch_size=64, shuffle=False)\n",
      "\n",
      "# Model and optimizer\n",
      "class SiameseNetwork(torch.nn.Module):\n",
      "    def __init__(self):\n",
      "        super(SiameseNetwork, self).__init__()\n",
      "        self.cnn1 = torch.nn.Sequential(\n",
      "            torch.nn.Conv2d(1, 32, kernel_size=5),\n",
      "            torch.nn.ReLU(),\n",
      "            torch.nn.MaxPool2d(kernel_size=2)\n",
      "        )\n",
      "        # Add more layers as needed\n",
      "        self.fc1 = torch.nn.Linear(32 * 61 * 61, 4096)\n",
      "\n",
      "    def forward_one(self, x):\n",
      "        output = self.cnn1(x)\n",
      "        output = output.view(output.size()[0], -1)\n",
      "        output = self.fc1(output)\n",
      "        return output\n",
      "\n",
      "    def forward(self, input1, input2):\n",
      "        output1 = self.forward_one(input1)\n",
      "        output2 = self.forward_one(input2)\n",
      "        return output1, output2\n",
      "\n",
      "model = SiameseNetwork().to(device)\n",
      "\n",
      "def contrastive_loss(embedding_a, embedding_b, label):\n",
      "    distance = torch.pairwise_distance(embedding_a, embedding_b)\n",
      "    loss_contrastive = torch.mean(label * torch.pow(distance, 2) + (1 - label) * torch.pow(torch.clamp(1.0 - distance, min=0.0), 2))\n",
      "    return loss_contrastive\n",
      "\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
      "\n",
      "def train_and_validate_siamese(model, dataloader, criterion, optimizer, epochs=5):\n",
      "    for epoch in range(epochs):\n",
      "        model.train()\n",
      "        running_loss = 0.0\n",
      "        for (img1, img2), label in dataloader:\n",
      "            img1, img2, label = img1.to(device).unsqueeze(1), img2.to(device).unsqueeze(1), label.to(device)\n",
      "            optimizer.zero_grad()\n",
      "            embedding_a, embedding_b = model(img1.float(), img2.float())\n",
      "            loss = criterion(embedding_a, embedding_b, label.float())\n",
      "            loss.backward()\n",
      "            optimizer.step()\n",
      "            running_loss += loss.item()\n",
      "\n",
      "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(dataloader)}\")\n",
      "\n",
      "def evaluate_siamese(model, dataloader):\n",
      "    model.eval()\n",
      "    all_labels = []\n",
      "    all_preds = []\n",
      "\n",
      "    with torch.no_grad():\n",
      "        for (img1, img2), label in dataloader:\n",
      "            img1, img2, label = img1.to(device).unsqueeze(1), img2.to(device).unsqueeze(1), label.to(device)\n",
      "\n",
      "            embedding_a, embedding_b = model(img1.float(), img2.float())\n",
      "            distance = torch.pairwise_distance(embedding_a, embedding_b)  # Ensure this line is correct\n",
      "\n",
      "            pred = (distance < 0.5).int().cpu()  # Convert distances to binary predictions\n",
      "            all_labels.extend(label.cpu().numpy())  # Collect labels\n",
      "            all_preds.extend(pred.numpy())  # Collect predictions\n",
      "\n",
      "    return accuracy_score(all_labels, all_preds), f1_score(all_labels, all_preds, average='macro')\n",
      "\n",
      "# Training and evaluation loop with KFold cross-validation\n",
      "kf = KFold(n_splits=5)\n",
      "for fold, (train_idx, val_idx) in enumerate(kf.split(siamese_train_dataset)):\n",
      "    print(f\"Training Fold {fold + 1}\")\n",
      "    train_loader_fold = DataLoader(SubsetRandomSampler(train_idx), batch_size=64, shuffle=True)\n",
      "    val_loader_fold = DataLoader(SubsetRandomSampler(val_idx), batch_size=64)\n",
      "\n",
      "    model = SiameseNetwork().to(device)\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
      "    \n",
      "    train_and_validate_siamese(model, train_loader_fold, contrastive_loss, optimizer, epochs=5)\n",
      "    \n",
      "    print(f\"Evaluation Fold {fold + 1}\")\n",
      "    acc, f1 = evaluate_siamese(model, val_loader_fold)\n",
      "    print(f\"Accuracy: {acc}, F1 Score: {f1}\")\n",
      "\n",
      "# Final evaluation on test set\n",
      "print(\"Final Evaluation\")\n",
      "acc, f1 = evaluate_siamese(model, test_loader)\n",
      "print(f\"Test Accuracy: {acc}, Test F1 Score: {f1}\")\n",
      "```\n",
      "\n",
      "This should ensure that the `evaluate_siamese` function correctly computes distances and predictions, avoiding any undefined variable issues. Make sure to adjust your model architecture as needed for better performance on MNIST data. The provided code includes a simple CNN structure; you may need to expand it depending on your requirements. \n",
      "\n",
      "### Final Notes:\n",
      "- Ensure that the dataset pairs are generated correctly (i.e., different labels).\n",
      "- Fine-tune hyperparameters and network architecture according to your needs.\n",
      "- Consider adding more layers or using pre-trained models for better results.\n",
      "\n",
      "This should resolve the `NameError` issue and ensure proper evaluation of your Siamese network. If you encounter further issues, please provide additional details! 🚀\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"\"\"\n",
    "esse codigo não roda me ajude, me entregue um codigo que funciona\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "UnboundLocalError                         Traceback (most recent call last)\n",
    "Cell In[28], line 52\n",
    "     50 for epoch in range(5):  # Treinamento por 5 épocas\n",
    "     51     model.train()\n",
    "---> 52     for images, labels in train_loader_fold:\n",
    "     53         images = images.to(device)\n",
    "     54         labels = labels.float().to(device)  # Converter para float\n",
    "\n",
    "File ~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733, in _BaseDataLoaderIter.__next__(self)\n",
    "    730 if self._sampler_iter is None:\n",
    "    731     # TODO(https://github.com/pytorch/pytorch/issues/76750)\n",
    "    732     self._reset()  # type: ignore[call-arg]\n",
    "--> 733 data = self._next_data()\n",
    "    734 self._num_yielded += 1\n",
    "    735 if (\n",
    "    736     self._dataset_kind == _DatasetKind.Iterable\n",
    "    737     and self._IterableDataset_len_called is not None\n",
    "    738     and self._num_yielded > self._IterableDataset_len_called\n",
    "    739 ):\n",
    "\n",
    "File ~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789, in _SingleProcessDataLoaderIter._next_data(self)\n",
    "    787 def _next_data(self):\n",
    "    788     index = self._next_index()  # may raise StopIteration\n",
    "--> 789     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
    "    790     if self._pin_memory:\n",
    "    791         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n",
    "\n",
    "File ~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52, in _MapDatasetFetcher.fetch(self, possibly_batched_index)\n",
    "     50         data = self.dataset.__getitems__(possibly_batched_index)\n",
    "     51     else:\n",
    "---> 52         data = [self.dataset[idx] for idx in possibly_batched_index]\n",
    "     53 else:\n",
    "     54     data = self.dataset[possibly_batched_index]\n",
    "\n",
    "File ~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52, in <listcomp>(.0)\n",
    "     50         data = self.dataset.__getitems__(possibly_batched_index)\n",
    "     51     else:\n",
    "---> 52         data = [self.dataset[idx] for idx in possibly_batched_index]\n",
    "     53 else:\n",
    "     54     data = self.dataset[possibly_batched_index]\n",
    "\n",
    "Cell In[25], line 74, in SiameseDataset.__getitem__(self, index)\n",
    "     71     img1 = self.transform(img1)\n",
    "     72     img2 = self.transform(img2)\n",
    "---> 74 return (img1, img2), int(similar_label)\n",
    "\n",
    "UnboundLocalError: cannot access local variable 'img2' where it is not associated with a value\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Transformação de dados\n",
    "transform = Resize((256, 256))\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=4096, n_filters=32):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # CNN backbone compartilhado\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, n_filters, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(n_filters, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        # Camada de embedding\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(1024, embedding_dim), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        output = self.cnn(x)\n",
    "        output = output.view(output.size()[0], -1)  # Flattening\n",
    "        output = self.embedding(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, input1, input2):\n",
    "        return self.forward_one(input1), self.forward_one(input2)\n",
    "\n",
    "def contrastive_loss(embedding_a, embedding_b, label, margin=1.0):\n",
    "    distance = torch.pairwise_distance(embedding_a, embedding_b)\n",
    "    \n",
    "    loss_contrastive = torch.mean((label) * torch.pow(distance, 2) +\n",
    "                                   (1 - label) * torch.pow(torch.clamp(margin - distance, min=0.0), 2))\n",
    "    return loss_contrastive\n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        self.mnist = mnist_dataset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img1, label1 = self.mnist[index]\n",
    "        \n",
    "        # Seleciona uma segunda imagem com a mesma classe ou diferente de acordo com o rótulo desejado.\n",
    "        if torch.rand(1) > 0.5:\n",
    "            similar_label = True\n",
    "            label2 = label1\n",
    "        else:\n",
    "            similar_label = False\n",
    "            while True:\n",
    "                index2 = torch.randint(len(self.mnist), (1,))\n",
    "                img2, label2 = self.mnist[index2]\n",
    "                if label2 != label1: break\n",
    "        \n",
    "        # Transformação de imagens para 256x256\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return (img1, img2), int(similar_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "\n",
    "def train_and_validate_siamese(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for (img1, img2), label in dataloader:\n",
    "            img1, img2, label = img1.to(device).unsqueeze(1), img2.to(device).unsqueeze(1), torch.tensor([label]).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            embedding_a, embedding_b = model(img1.float(), img2.float())\n",
    "            loss = criterion(embedding_a, embedding_b, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * len(label)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs} - Loss: {running_loss / len(dataloader.dataset):.4f}')\n",
    "\n",
    "def evaluate_siamese(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (img1, img2), label in dataloader:\n",
    "            img1, img2, label = img1.to(device).unsqueeze(1), img2.to(device).unsqueeze(1), torch.tensor([label]).to(device)\n",
    "            \n",
    "            embedding_a, embedding_b = model(img1.float(), img2.float())\n",
    "            distance = torch.pairwise_distance(embedding_a, embedding_b)\n",
    "\n",
    "            pred = (distance < 0.5).int().cpu()\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_preds.extend(pred.numpy())\n",
    "\n",
    "    return accuracy_score(all_labels, all_preds), f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "# Configuração do dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Carregamento dos dados MNIST\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Redimensionamento das imagens para 256x256\n",
    "train_dataset.transform = transform\n",
    "test_dataset.transform = transform\n",
    "\n",
    "# Criação do dataset personalizado e carregamento em DataLoader\n",
    "siamese_train_dataset = SiameseDataset(train_dataset)\n",
    "siamese_test_dataset = SiameseDataset(test_dataset)\n",
    "\n",
    "train_loader = DataLoader(siamese_train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(siamese_test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Inicialização do modelo e otimizador\n",
    "model = SiameseNetwork().to(device)\n",
    "criterion = contrastive_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Treinamento e validação com k-fold cross-validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(siamese_train_dataset)):\n",
    "    print(f'Fold {fold + 1}/{k_folds}')\n",
    "    \n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_loader_fold = DataLoader(siamese_train_dataset, batch_size=64, sampler=train_sampler)\n",
    "    val_loader_fold = DataLoader(siamese_train_dataset, batch_size=64, sampler=valid_sampler)\n",
    "\n",
    "    model = SiameseNetwork().to(device)  # Reinicializa o modelo para cada fold\n",
    "    criterion = contrastive_loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_and_validate_siamese(model, train_loader_fold, criterion, optimizer, epochs=5)\n",
    "\n",
    "# Avaliação final no conjunto de teste\n",
    "accuracy, f1_score_ = evaluate_siamese(model, test_loader)\n",
    "print(f'Test Accuracy: {accuracy:.4f}, F1 Score: {f1_score_:.4f}')\n",
    "\n",
    "# Análise dos resultados e comparação de hiperparâmetros (exemplo simplificado)\n",
    "best_model_params = {'lr': 0.001, 'epochs': 5}\n",
    "second_best_model_params = {'lr': 0.0001, 'epochs': 10}\n",
    "\n",
    "# Supondo que os resultados foram armazenados\n",
    "results_best_model = (accuracy, f1_score_)\n",
    "results_second_best_model = (accuracy - 0.02, f1_score_ - 0.01)\n",
    "\n",
    "statistic, p_value = scipy.stats.ttest_ind(results_best_model[1], results_second_best_model[1])\n",
    "print(f'p-value: {p_value}')\n",
    "if p_value < 0.05:\n",
    "    print('There is a statistically significant difference between the models.')\n",
    "else:\n",
    "    print('No statistically significant difference found.')\n",
    "\"\"\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a493874-dd49-4f72-96aa-a20c1e2efa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6b7414b-2fee-4a42-94e7-6f1b87e46237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 1, 1, 256, 256]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 128\u001b[39m\n\u001b[32m    125\u001b[39m model = SiameseNetwork().to(device)\n\u001b[32m    126\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[43mtrain_and_validate_siamese\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrastive_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluation Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    131\u001b[39m acc, f1 = evaluate_siamese(model, val_loader_fold)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mtrain_and_validate_siamese\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, epochs)\u001b[39m\n\u001b[32m     88\u001b[39m img1, img2, label = img1.to(device).unsqueeze(\u001b[32m1\u001b[39m), img2.to(device).unsqueeze(\u001b[32m1\u001b[39m), label.to(device).float() \u001b[38;5;66;03m# Convert label to float for loss calculation\u001b[39;00m\n\u001b[32m     89\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m embedding_a, embedding_b = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m loss = criterion(embedding_a, embedding_b, label)\n\u001b[32m     92\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mSiameseNetwork.forward\u001b[39m\u001b[34m(self, input1, input2)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input1, input2):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     output1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     output2 = \u001b[38;5;28mself\u001b[39m.forward_one(input2)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output1, output2\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mSiameseNetwork.forward_one\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcnn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     output = output.view(output.size()[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n\u001b[32m     66\u001b[39m     output = \u001b[38;5;28mself\u001b[39m.fc1(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/agentsLang/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 1, 1, 256, 256]"
     ]
    }
   ],
   "source": [
    "def evaluate_siamese(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (img1, img2), label in dataloader:\n",
    "            img1, img2, label = img1.to(device).unsqueeze(1), img2.to(device).unsqueeze(1), torch.tensor([label]).to(device)\n",
    "\n",
    "            embedding_a, embedding_b = model(img1.float(), img2.float())\n",
    "            distance = torch.pairwise_distance(embedding_a, embedding_b)  # Ensure this line is correct\n",
    "\n",
    "            pred = (distance < 0.5).int().cpu()  # Convert distances to binary predictions\n",
    "            all_labels.extend(label.cpu().numpy())  # Collect labels\n",
    "            all_preds.extend(pred.numpy())  # Collect predictions\n",
    "\n",
    "    return accuracy_score(all_labels, all_preds), f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data loading and preprocessing\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Custom dataset for Siamese network\n",
    "class SiameseDataset:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1, label1 = self.dataset[idx]\n",
    "        while True:\n",
    "            img2, label2 = self.dataset[torch.randint(len(self.dataset), (1,)).item()]\n",
    "            if label1 != label2:  # Ensure different labels\n",
    "                break\n",
    "        return img1, img2, int(label1 == label2)\n",
    "\n",
    "# Custom dataset for Siamese network with resizing\n",
    "train_dataset.transform = transforms.Compose([transforms.Resize((256, 256)), train_dataset.transform])\n",
    "test_dataset.transform = transforms.Compose([transforms.Resize((256, 256)), test_dataset.transform])\n",
    "\n",
    "siamese_train_dataset = SiameseDataset(train_dataset)\n",
    "siamese_test_dataset = SiameseDataset(test_dataset)\n",
    "\n",
    "train_loader = DataLoader(siamese_train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(siamese_test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Model and optimizer\n",
    "class SiameseNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        # Add more layers as needed\n",
    "        self.fc1 = torch.nn.Linear(32 * 61 * 61, 4096)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2\n",
    "\n",
    "model = SiameseNetwork().to(device)\n",
    "\n",
    "def contrastive_loss(embedding_a, embedding_b, label):\n",
    "    distance = torch.pairwise_distance(embedding_a, embedding_b)\n",
    "    loss_contrastive = torch.mean(label * torch.pow(distance, 2) + (1 - label) * torch.pow(torch.clamp(1.0 - distance, min=0.0), 2))\n",
    "    return loss_contrastive\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_and_validate_siamese(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for img1, img2, label in dataloader:  # Modifiquei aqui para receber três valores\n",
    "            img1, img2, label = img1.to(device).unsqueeze(1), img2.to(device).unsqueeze(1), label.to(device).float() # Convert label to float for loss calculation\n",
    "            optimizer.zero_grad()\n",
    "            embedding_a, embedding_b = model(img1.float(), img2.float())\n",
    "            loss = criterion(embedding_a, embedding_b, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(dataloader)}\")\n",
    "\n",
    "def evaluate_siamese(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, label in dataloader: # Modifiquei aqui para receber três valores\n",
    "            img1, img2, label = img1.to(device).unsqueeze(1), img2.to(device).unsqueeze(1), label.to(device)\n",
    "\n",
    "            embedding_a, embedding_b = model(img1.float(), img2.float())\n",
    "            distance = torch.pairwise_distance(embedding_a, embedding_b)\n",
    "\n",
    "            pred = (distance < 0.5).int().cpu()\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_preds.extend(pred.numpy())\n",
    "\n",
    "    return accuracy_score(all_labels, all_preds), f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "# Training and evaluation loop with KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42) # Adicionei shuffle e random_state para reprodutibilidade\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(siamese_train_dataset)):\n",
    "    print(f\"Training Fold {fold + 1}\")\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader_fold = DataLoader(siamese_train_dataset, batch_size=64, sampler=train_sampler)\n",
    "    val_loader_fold = DataLoader(siamese_train_dataset, batch_size=64, sampler=val_sampler)\n",
    "\n",
    "    model = SiameseNetwork().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_and_validate_siamese(model, train_loader_fold, contrastive_loss, optimizer, epochs=5)\n",
    "\n",
    "    print(f\"Evaluation Fold {fold + 1}\")\n",
    "    acc, f1 = evaluate_siamese(model, val_loader_fold)\n",
    "    print(f\"Accuracy: {acc}, F1 Score: {f1}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "print(\"Final Evaluation\")\n",
    "acc, f1 = evaluate_siamese(model, test_loader)\n",
    "print(f\"Test Accuracy: {acc}, Test F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b461900-d549-4e79-8e35-0b91dc601b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 116\u001b[39m\n\u001b[32m    113\u001b[39m model = SiameseNetwork().to(device)\n\u001b[32m    114\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[43mtrain_and_validate_siamese\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrastive_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluation Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m acc, f1 = evaluate_siamese(model, val_loader_fold)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mtrain_and_validate_siamese\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, epochs)\u001b[39m\n\u001b[32m     73\u001b[39m model.train()\n\u001b[32m     74\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (img1, img2), label \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[32m     76\u001b[39m     img1, img2, label = img1.to(device).unsqueeze(\u001b[32m1\u001b[39m), img2.to(device).unsqueeze(\u001b[32m1\u001b[39m), label.to(device)\n\u001b[32m     77\u001b[39m     optimizer.zero_grad()\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data loading and preprocessing\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Custom dataset for Siamese network\n",
    "class SiameseDataset:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1, label1 = self.dataset[idx]\n",
    "        while True:\n",
    "            img2, label2 = self.dataset[torch.randint(len(self.dataset), (1,)).item()]\n",
    "            if label1 != label2:  # Ensure different labels\n",
    "                break\n",
    "        return img1, img2, int(label1 == label2)\n",
    "\n",
    "# Custom dataset for Siamese network with resizing\n",
    "train_dataset.transform = transforms.Compose([transforms.Resize((256, 256)), train_dataset.transform])\n",
    "test_dataset.transform = transforms.Compose([transforms.Resize((256, 256)), test_dataset.transform])\n",
    "\n",
    "siamese_train_dataset = SiameseDataset(train_dataset)\n",
    "siamese_test_dataset = SiameseDataset(test_dataset)\n",
    "\n",
    "train_loader = DataLoader(siamese_train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(siamese_test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Model and optimizer\n",
    "class SiameseNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        # Add more layers as needed\n",
    "        self.fc1 = torch.nn.Linear(32 * 61 * 61, 4096)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2\n",
    "\n",
    "model = SiameseNetwork().to(device)\n",
    "\n",
    "def contrastive_loss(embedding_a, embedding_b, label):\n",
    "    distance = torch.pairwise_distance(embedding_a, embedding_b)\n",
    "    loss_contrastive = torch.mean(label * torch.pow(distance, 2) + (1 - label) * torch.pow(torch.clamp(1.0 - distance, min=0.0), 2))\n",
    "    return loss_contrastive\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_and_validate_siamese(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for (img1, img2), label in dataloader:\n",
    "            img1, img2, label = img1.to(device).unsqueeze(1), img2.to(device).unsqueeze(1), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            embedding_a, embedding_b = model(img1.float(), img2.float())\n",
    "            loss = criterion(embedding_a, embedding_b, label.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(dataloader)}\")\n",
    "\n",
    "def evaluate_siamese(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (img1, img2), label in dataloader:\n",
    "            img1, img2, label = img1.to(device).unsqueeze(1), img2.to(device).unsqueeze(1), label.to(device)\n",
    "\n",
    "            embedding_a, embedding_b = model(img1.float(), img2.float())\n",
    "            distance = torch.pairwise_distance(embedding_a, embedding_b)  # Ensure this line is correct\n",
    "\n",
    "            pred = (distance < 0.5).int().cpu()  # Convert distances to binary predictions\n",
    "            all_labels.extend(label.cpu().numpy())  # Collect labels\n",
    "            all_preds.extend(pred.numpy())  # Collect predictions\n",
    "\n",
    "    return accuracy_score(all_labels, all_preds), f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "# Training and evaluation loop with KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42) # Adicionei shuffle e random_state para reprodutibilidade\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(siamese_train_dataset)):\n",
    "    print(f\"Training Fold {fold + 1}\")\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader_fold = DataLoader(siamese_train_dataset, batch_size=64, sampler=train_sampler)\n",
    "    val_loader_fold = DataLoader(siamese_train_dataset, batch_size=64, sampler=val_sampler)\n",
    "\n",
    "    model = SiameseNetwork().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_and_validate_siamese(model, train_loader_fold, contrastive_loss, optimizer, epochs=5)\n",
    "\n",
    "    print(f\"Evaluation Fold {fold + 1}\")\n",
    "    acc, f1 = evaluate_siamese(model, val_loader_fold)\n",
    "    print(f\"Accuracy: {acc}, F1 Score: {f1}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "print(\"Final Evaluation\")\n",
    "acc, f1 = evaluate_siamese(model, test_loader)\n",
    "print(f\"Test Accuracy: {acc}, Test F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea61011-d4f0-4fa2-825b-5a19ed016f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
